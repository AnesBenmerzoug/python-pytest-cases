{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytest-cases \u00b6 Separate test code from test cases in pytest . New unpacking feature , check it out ! Test execution order Installing pytest-cases now has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it . Did you ever think that most of your test functions were actually the same test code , but with different data inputs and expected results/exceptions ? pytest-cases leverages pytest and its great @pytest.mark.parametrize decorator, so that you can separate your test cases from your test functions . For example with pytest-cases you can now write your tests with the following pattern: on one hand, the usual test_xxxx.py file containing your test functions on the other hand, a new test_xxxx_cases.py containing your cases functions pytest-cases is fully compliant with pytest-steps so you can create test suites with several steps and send each case on the full suite. See usage page for details . Installing \u00b6 > pip install pytest_cases Usage - 'Data' cases \u00b6 a- Some code to test \u00b6 Let's consider the following foo function under test: def foo ( a , b ): return a + 1 , b + 1 b- Case functions \u00b6 First we create a test_foo_cases.py file. This file will contain test cases generator functions, that we will call case functions for brevity: def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return dict ( a = 1 , b = 2 ) def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return dict ( a =- 1 , b =- 2 ) In these functions, you will typically either parse some test data files, or generate some simulated test data and expected results. Case functions do not have any particular requirement , apart from their names starting with case_ . They can return anything that is considered useful to run the associated test. Support for pytest marks Pytest marks such as @pytest.mark.skip can be used on case functions, the corresponding case will be handled according to the expected behaviour (failed if @pytest.mark.fail , skipped under condition if @pytest.mark.skipif , etc.) c- Test functions \u00b6 Then, as usual we write our pytest functions starting with test_ , in a test_foo.py file: from pytest_cases import cases_data from example import foo # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_foo ( case_data ): \"\"\" Example unit test that is automatically parametrized with @cases_data \"\"\" # 1- Grab the test case data inputs = case_data . get () # 2- Use it foo ( ** inputs ) Note: as explained here , cases can also be located inside the test file. As you can see above there are three things that are needed to parametrize a test function with associated case functions: decorate your test function with @cases_data , indicating which module contains the cases functions add an input argument to your test function, named case_data with optional type hint CaseData use that input argument at the beginning of the test function, to retrieve the test data: inputs = case_data.get() Once you have done these three steps, executing pytest will run your test function once for every case function : >>> pytest ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ case_two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ case_two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ========================== d- Case fixtures \u00b6 You might be concerned that case data is gathered or created during test execution. Indeed creating or collecting case data is not part of the test per se . Besides, if you benchmark your tests durations (for example with pytest-harvest ), you may want the test duration to be computed without acccounting for the data retrieval time - especially if you decide to add some caching mechanism as explained here . It might therefore be more interesting for you to parametrize case fixtures instead of parametrizing your test function. Thanks to our new @pytest_fixture_plus decorator, this works exactly the same way than for test functions: from pytest_cases import pytest_fixture_plus , cases_data from example import foo # import the module containing the test cases import test_foo_cases @pytest_fixture_plus @cases_data ( module = test_foo_cases ) def inputs ( case_data ): \"\"\" Example fixture that is automatically parametrized with @cases_data \"\"\" # retrieve case data return case_data . get () def test_foo ( inputs ): # Use case data foo ( ** inputs ) In the above example, the test_foo test does not spend time collecting or generating data. When it is executed, it receives the required data directly as inputs . The test case creation instead happens when each inputs fixture instance is created by pytest - this is done in a separate pytest phase (named \"setup\"), and therefore is not counted in the test duration. Note: you can still use request in your fixture's signature if you wish to. Usage - 'True' test cases \u00b6 a- Case functions update \u00b6 In the above example the cases were only containing inputs for the function to test. In real-world applications we often need more: we need both inputs and an expected outcome . For this, pytest_cases proposes to adopt a convention where the case functions returns a tuple of inputs/outputs/errors. A handy CaseData PEP484 type hint can be used to denote that. But of course this is only a proposal, which is not mandatory as we saw above. A case function can return anything Even if in most examples in this documentation we chose to return a tuple (inputs/outputs/errors) (type hint CaseData ), you can decide to return anything: a single variable, a dictionary, a tuple of a different length, etc. Whatever you return will be available through case_data.get() . Here is how we can rewrite our case functions with an expected outcome: def case_two_positive_ints () -> CaseData : \"\"\" Inputs are two positive integers \"\"\" ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None def case_two_negative_ints () -> CaseData : \"\"\" Inputs are two negative integers \"\"\" ins = dict ( a =- 1 , b =- 2 ) outs = 0 , - 1 return ins , outs , None We propose that the \"expected error\" ( None above) may contain exception type, exception instances, or callables. If you follow this convention, you will be able to write your test more easily with the provided utility function unfold_expected_err . See here for details . b- Test body update \u00b6 With our new case functions, a case will be made of three items. So case_data.get() will return a tuple. Here is how we can update our test function body to retrieve it correctly, and check that the outcome is as expected: @cases_data ( module = test_foo_cases ) def test_foo ( case_data : CaseDataGetter ): \"\"\" Example unit test that is automatically parametrized with @cases_data \"\"\" # 1- Grab the test case data: now a tuple ! i , expected_o , expected_e = case_data . get () # 2- Use it: we can now do some asserts ! if expected_e is None : # **** Nominal test **** outs = foo ( ** i ) assert outs == expected_o else : # **** Error tests: see <Usage> page to fill this **** pass See Usage for complete examples with custom case names, case generators, exceptions handling, and more. pytest Goodies \u00b6 --with-reorder \u00b6 pytest postprocesses the order of the collected items in order to optimize setup/teardown of session, module and class fixtures. This optimization algorithm happens at the pytest_collection_modifyitems stage, and is still under improvement, as can be seen in pytest#3551 , pytest#3393 , #2846 ... Besides other plugins such as pytest-reorder can modify the order as well. This new commandline is a goodie to change the reordering: --with-reorder normal is the default behaviour: it lets pytest and all the plugins execute their reordering in each of their pytest_collection_modifyitems hooks, and simply does not interact --with-reorder skip allows you to restore the original order that was active before pytest_collection_modifyitems was initially called, thus not taking into account any reordering done by pytest or by any of its plugins. @pytest_fixture_plus \u00b6 @pytest_fixture_plus is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. As a bonus, its name argument works even in old versions of pytest (which is not the case for fixture ). Finally it now supports unpacking, see unpacking feature . @pytest_fixture_plus deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @pytest_fixture_plus will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases. unpack_fixture / unpack_into \u00b6 In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , pytest_fixture_plus @pytest_fixture_plus @pytest.mark.parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @pytest_fixture_plus to do the same thing: import pytest from pytest_cases import pytest_fixture_plus @pytest_fixture_plus ( unpack_into = \"a,b\" ) @pytest.mark.parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import pytest_fixture_plus , fixture_union @pytest_fixture_plus @pytest.mark.parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @pytest_fixture_plus @pytest.mark.parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b param_fixture[s] \u00b6 If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest.fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest.fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ... fixture_union \u00b6 As of pytest 4, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that will first take all the possible values of fixture A, then all possible values of fixture B, etc. The topic has been largely discussed in pytest-dev#349 and a request for proposal has been finally made. fixture_union is an implementation of this proposal. from pytest_cases import pytest_fixture_plus , fixture_union @pytest_fixture_plus def first (): return 'hello' @pytest_fixture_plus ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED As you can see the ids of union fixtures are slightly different from standard ids, so that you can easily understand what is going on. You can change this feature with \u00ecdstyle , see API documentation for details. This feature has been tested in very complex cases (several union fixtures, fixtures that are not selected by a given union but that is requested by the test function, etc.). But if you find some strange behaviour don't hesitate to report it in the issues page ! IMPORTANT if you do not use @pytest_fixture_plus but only @pytest.fixture , then you will see that your fixtures are called even when they are not used, with a parameter NOT_USED . This symbol is automatically ignored if you use @pytest_fixture_plus , otherwise you have to handle it. fixture unions vs. cases If you're familiar with pytest-cases already, you might note @cases_data is not so different than a fixture union: we do a union of all case functions. If one day union fixtures are directly supported by pytest , we will probably refactor this lib to align all the concepts. Finally fixture unions now supports unpacking, see unpacking feature . @pytest_parametrize_plus \u00b6 @pytest_parametrize_plus is a replacement for @pytest.mark.parametrize that allows you to include references to fixtures in the parameter values. Simply use fixture_ref(<fixture>) in the parameter values, where <fixture> can be the fixture name or fixture function. For example: import pytest from pytest_cases import pytest_parametrize_plus , pytest_fixture_plus , fixture_ref @pytest.fixture def world_str (): return 'world' @pytest_fixture_plus @pytest_parametrize_plus ( 'who' , [ fixture_ref ( world_str ), 'you' ]) def greetings ( who ): return 'hello ' + who @pytest_parametrize_plus ( 'main_msg' , [ 'nothing' , fixture_ref ( world_str ), fixture_ref ( greetings )]) @pytest.mark.parametrize ( 'ending' , [ '?' , '!' ]) def test_prints ( main_msg , ending ): print ( main_msg + ending ) yields the following > pytest -s -v collected 9 items test_prints [ test_prints_main_msg_is_0-nothing-? ] nothing? PASSED test_prints [ test_prints_main_msg_is_0-nothing-! ] nothing! PASSED test_prints [ test_prints_main_msg_is_world_str-? ] world? PASSED test_prints [ test_prints_main_msg_is_world_str-! ] world! PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_world_str-? ] hello world? PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_world_str-! ] hello world! PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_1-you-? ] hello you? PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_1-you-! ] hello you! PASSED As you can see, the ids are a bit more explicit than usual. As opposed to fixture_union , the style of these ids is not configurable for now but feel free to propose alternatives in the issues page . Note: for this to be performed, the parameters are replaced with a union fixture. Therefore the relative priority order of these parameters with other standard pytest.mark.parametrize parameters that you would place on the same function, will get impacted. You may solve this by replacing your mark parameters with param_fixture s (see above .) Main features / benefits \u00b6 Separation of concerns : test code on one hand, test cases data on the other hand. This is particularly relevant for data science projects where a lot of test datasets are used on the same block of test code. Everything in the test or in the fixture , not outside. A side-effect of @pytest.mark.parametrize is that users tend to create or parse their datasets outside of the test function. pytest_cases suggests a model where the potentially time and memory consuming step of case data generation/retrieval is performed inside the test node or the required fixture, thus keeping every test case run more independent. It is also easy to put debug breakpoints on specific test cases. Easier iterable-based test case generation . If you wish to generate several test cases using the same function, @cases_generator makes it very intuitive to do so. See here for details. User-friendly features : easily customize your test cases with friendly names, reuse the same cases for different test functions by tagging/filtering, and more... See Usage for details. See Also \u00b6 pytest documentation on parametrize pytest documentation on fixtures pytest-steps pytest-harvest Others \u00b6 Do you like this library ? You might also like my other python libraries Want to contribute ? \u00b6 Details on the github page: https://github.com/smarie/python-pytest-cases","title":"Home"},{"location":"#pytest-cases","text":"Separate test code from test cases in pytest . New unpacking feature , check it out ! Test execution order Installing pytest-cases now has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it . Did you ever think that most of your test functions were actually the same test code , but with different data inputs and expected results/exceptions ? pytest-cases leverages pytest and its great @pytest.mark.parametrize decorator, so that you can separate your test cases from your test functions . For example with pytest-cases you can now write your tests with the following pattern: on one hand, the usual test_xxxx.py file containing your test functions on the other hand, a new test_xxxx_cases.py containing your cases functions pytest-cases is fully compliant with pytest-steps so you can create test suites with several steps and send each case on the full suite. See usage page for details .","title":"pytest-cases"},{"location":"#installing","text":"> pip install pytest_cases","title":"Installing"},{"location":"#usage-data-cases","text":"","title":"Usage - 'Data' cases"},{"location":"#a-some-code-to-test","text":"Let's consider the following foo function under test: def foo ( a , b ): return a + 1 , b + 1","title":"a- Some code to test"},{"location":"#b-case-functions","text":"First we create a test_foo_cases.py file. This file will contain test cases generator functions, that we will call case functions for brevity: def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return dict ( a = 1 , b = 2 ) def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return dict ( a =- 1 , b =- 2 ) In these functions, you will typically either parse some test data files, or generate some simulated test data and expected results. Case functions do not have any particular requirement , apart from their names starting with case_ . They can return anything that is considered useful to run the associated test. Support for pytest marks Pytest marks such as @pytest.mark.skip can be used on case functions, the corresponding case will be handled according to the expected behaviour (failed if @pytest.mark.fail , skipped under condition if @pytest.mark.skipif , etc.)","title":"b- Case functions"},{"location":"#c-test-functions","text":"Then, as usual we write our pytest functions starting with test_ , in a test_foo.py file: from pytest_cases import cases_data from example import foo # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_foo ( case_data ): \"\"\" Example unit test that is automatically parametrized with @cases_data \"\"\" # 1- Grab the test case data inputs = case_data . get () # 2- Use it foo ( ** inputs ) Note: as explained here , cases can also be located inside the test file. As you can see above there are three things that are needed to parametrize a test function with associated case functions: decorate your test function with @cases_data , indicating which module contains the cases functions add an input argument to your test function, named case_data with optional type hint CaseData use that input argument at the beginning of the test function, to retrieve the test data: inputs = case_data.get() Once you have done these three steps, executing pytest will run your test function once for every case function : >>> pytest ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ case_two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ case_two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ==========================","title":"c- Test functions"},{"location":"#d-case-fixtures","text":"You might be concerned that case data is gathered or created during test execution. Indeed creating or collecting case data is not part of the test per se . Besides, if you benchmark your tests durations (for example with pytest-harvest ), you may want the test duration to be computed without acccounting for the data retrieval time - especially if you decide to add some caching mechanism as explained here . It might therefore be more interesting for you to parametrize case fixtures instead of parametrizing your test function. Thanks to our new @pytest_fixture_plus decorator, this works exactly the same way than for test functions: from pytest_cases import pytest_fixture_plus , cases_data from example import foo # import the module containing the test cases import test_foo_cases @pytest_fixture_plus @cases_data ( module = test_foo_cases ) def inputs ( case_data ): \"\"\" Example fixture that is automatically parametrized with @cases_data \"\"\" # retrieve case data return case_data . get () def test_foo ( inputs ): # Use case data foo ( ** inputs ) In the above example, the test_foo test does not spend time collecting or generating data. When it is executed, it receives the required data directly as inputs . The test case creation instead happens when each inputs fixture instance is created by pytest - this is done in a separate pytest phase (named \"setup\"), and therefore is not counted in the test duration. Note: you can still use request in your fixture's signature if you wish to.","title":"d- Case fixtures"},{"location":"#usage-true-test-cases","text":"","title":"Usage - 'True' test cases"},{"location":"#a-case-functions-update","text":"In the above example the cases were only containing inputs for the function to test. In real-world applications we often need more: we need both inputs and an expected outcome . For this, pytest_cases proposes to adopt a convention where the case functions returns a tuple of inputs/outputs/errors. A handy CaseData PEP484 type hint can be used to denote that. But of course this is only a proposal, which is not mandatory as we saw above. A case function can return anything Even if in most examples in this documentation we chose to return a tuple (inputs/outputs/errors) (type hint CaseData ), you can decide to return anything: a single variable, a dictionary, a tuple of a different length, etc. Whatever you return will be available through case_data.get() . Here is how we can rewrite our case functions with an expected outcome: def case_two_positive_ints () -> CaseData : \"\"\" Inputs are two positive integers \"\"\" ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None def case_two_negative_ints () -> CaseData : \"\"\" Inputs are two negative integers \"\"\" ins = dict ( a =- 1 , b =- 2 ) outs = 0 , - 1 return ins , outs , None We propose that the \"expected error\" ( None above) may contain exception type, exception instances, or callables. If you follow this convention, you will be able to write your test more easily with the provided utility function unfold_expected_err . See here for details .","title":"a- Case functions update"},{"location":"#b-test-body-update","text":"With our new case functions, a case will be made of three items. So case_data.get() will return a tuple. Here is how we can update our test function body to retrieve it correctly, and check that the outcome is as expected: @cases_data ( module = test_foo_cases ) def test_foo ( case_data : CaseDataGetter ): \"\"\" Example unit test that is automatically parametrized with @cases_data \"\"\" # 1- Grab the test case data: now a tuple ! i , expected_o , expected_e = case_data . get () # 2- Use it: we can now do some asserts ! if expected_e is None : # **** Nominal test **** outs = foo ( ** i ) assert outs == expected_o else : # **** Error tests: see <Usage> page to fill this **** pass See Usage for complete examples with custom case names, case generators, exceptions handling, and more.","title":"b- Test body update"},{"location":"#pytest-goodies","text":"","title":"pytest Goodies"},{"location":"#-with-reorder","text":"pytest postprocesses the order of the collected items in order to optimize setup/teardown of session, module and class fixtures. This optimization algorithm happens at the pytest_collection_modifyitems stage, and is still under improvement, as can be seen in pytest#3551 , pytest#3393 , #2846 ... Besides other plugins such as pytest-reorder can modify the order as well. This new commandline is a goodie to change the reordering: --with-reorder normal is the default behaviour: it lets pytest and all the plugins execute their reordering in each of their pytest_collection_modifyitems hooks, and simply does not interact --with-reorder skip allows you to restore the original order that was active before pytest_collection_modifyitems was initially called, thus not taking into account any reordering done by pytest or by any of its plugins.","title":"--with-reorder"},{"location":"#pytest_fixture_plus","text":"@pytest_fixture_plus is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. As a bonus, its name argument works even in old versions of pytest (which is not the case for fixture ). Finally it now supports unpacking, see unpacking feature . @pytest_fixture_plus deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @pytest_fixture_plus will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases.","title":"@pytest_fixture_plus"},{"location":"#unpack_fixture-unpack_into","text":"In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , pytest_fixture_plus @pytest_fixture_plus @pytest.mark.parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @pytest_fixture_plus to do the same thing: import pytest from pytest_cases import pytest_fixture_plus @pytest_fixture_plus ( unpack_into = \"a,b\" ) @pytest.mark.parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import pytest_fixture_plus , fixture_union @pytest_fixture_plus @pytest.mark.parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @pytest_fixture_plus @pytest.mark.parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b","title":"unpack_fixture / unpack_into"},{"location":"#param_fixtures","text":"If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest.fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest.fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ...","title":"param_fixture[s]"},{"location":"#fixture_union","text":"As of pytest 4, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that will first take all the possible values of fixture A, then all possible values of fixture B, etc. The topic has been largely discussed in pytest-dev#349 and a request for proposal has been finally made. fixture_union is an implementation of this proposal. from pytest_cases import pytest_fixture_plus , fixture_union @pytest_fixture_plus def first (): return 'hello' @pytest_fixture_plus ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED As you can see the ids of union fixtures are slightly different from standard ids, so that you can easily understand what is going on. You can change this feature with \u00ecdstyle , see API documentation for details. This feature has been tested in very complex cases (several union fixtures, fixtures that are not selected by a given union but that is requested by the test function, etc.). But if you find some strange behaviour don't hesitate to report it in the issues page ! IMPORTANT if you do not use @pytest_fixture_plus but only @pytest.fixture , then you will see that your fixtures are called even when they are not used, with a parameter NOT_USED . This symbol is automatically ignored if you use @pytest_fixture_plus , otherwise you have to handle it. fixture unions vs. cases If you're familiar with pytest-cases already, you might note @cases_data is not so different than a fixture union: we do a union of all case functions. If one day union fixtures are directly supported by pytest , we will probably refactor this lib to align all the concepts. Finally fixture unions now supports unpacking, see unpacking feature .","title":"fixture_union"},{"location":"#pytest_parametrize_plus","text":"@pytest_parametrize_plus is a replacement for @pytest.mark.parametrize that allows you to include references to fixtures in the parameter values. Simply use fixture_ref(<fixture>) in the parameter values, where <fixture> can be the fixture name or fixture function. For example: import pytest from pytest_cases import pytest_parametrize_plus , pytest_fixture_plus , fixture_ref @pytest.fixture def world_str (): return 'world' @pytest_fixture_plus @pytest_parametrize_plus ( 'who' , [ fixture_ref ( world_str ), 'you' ]) def greetings ( who ): return 'hello ' + who @pytest_parametrize_plus ( 'main_msg' , [ 'nothing' , fixture_ref ( world_str ), fixture_ref ( greetings )]) @pytest.mark.parametrize ( 'ending' , [ '?' , '!' ]) def test_prints ( main_msg , ending ): print ( main_msg + ending ) yields the following > pytest -s -v collected 9 items test_prints [ test_prints_main_msg_is_0-nothing-? ] nothing? PASSED test_prints [ test_prints_main_msg_is_0-nothing-! ] nothing! PASSED test_prints [ test_prints_main_msg_is_world_str-? ] world? PASSED test_prints [ test_prints_main_msg_is_world_str-! ] world! PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_world_str-? ] hello world? PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_world_str-! ] hello world! PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_1-you-? ] hello you? PASSED test_prints [ test_prints_main_msg_is_greetings-greetings_who_is_1-you-! ] hello you! PASSED As you can see, the ids are a bit more explicit than usual. As opposed to fixture_union , the style of these ids is not configurable for now but feel free to propose alternatives in the issues page . Note: for this to be performed, the parameters are replaced with a union fixture. Therefore the relative priority order of these parameters with other standard pytest.mark.parametrize parameters that you would place on the same function, will get impacted. You may solve this by replacing your mark parameters with param_fixture s (see above .)","title":"@pytest_parametrize_plus"},{"location":"#main-features-benefits","text":"Separation of concerns : test code on one hand, test cases data on the other hand. This is particularly relevant for data science projects where a lot of test datasets are used on the same block of test code. Everything in the test or in the fixture , not outside. A side-effect of @pytest.mark.parametrize is that users tend to create or parse their datasets outside of the test function. pytest_cases suggests a model where the potentially time and memory consuming step of case data generation/retrieval is performed inside the test node or the required fixture, thus keeping every test case run more independent. It is also easy to put debug breakpoints on specific test cases. Easier iterable-based test case generation . If you wish to generate several test cases using the same function, @cases_generator makes it very intuitive to do so. See here for details. User-friendly features : easily customize your test cases with friendly names, reuse the same cases for different test functions by tagging/filtering, and more... See Usage for details.","title":"Main features / benefits"},{"location":"#see-also","text":"pytest documentation on parametrize pytest documentation on fixtures pytest-steps pytest-harvest","title":"See Also"},{"location":"#others","text":"Do you like this library ? You might also like my other python libraries","title":"Others"},{"location":"#want-to-contribute","text":"Details on the github page: https://github.com/smarie/python-pytest-cases","title":"Want to contribute ?"},{"location":"api_reference/","text":"API reference \u00b6 In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package. 1 - On case functions side \u00b6 CaseData type hint \u00b6 A proposed standard type hint for the case functions outputs. CaseData = Tuple[Given, ExpectedNormal, ExpectedError] where Given = Any \"\"\"The input(s) for the test. It can be anything\"\"\" ExpectedNormal = Optional [ Any ] \"\"\"The expected test results in case success is expected, or None if this test should fail\"\"\" ExpectedError = Optional [ Union [ Type [ Exception ], Exception , Callable [[ Exception ], Optional [ bool ]]]] \"\"\"The expected error in case failure is expected, or None if the test should succeed. It is proposed that expected error can be defined as an exception type, an exception instance, or an exception validation function\"\"\" @case_name \u00b6 @case_name(name: str) Decorator to override the name of a case function. The new name will be used instead of the function name, in test names. @case_name ( 'simple_case' ) def case_simple (): ... @case_tags \u00b6 @case_tags(*tags: Any) Decorator to tag a case function with a list of tags. These tags can then be used in the @cases_data test function decorator to filter cases within the selected module(s). Parameters: tags : a list of tags that may be used to filter the case. Tags can be anything (string, objects, types, functions...) @test_target \u00b6 @test_target(target: Any) A simple decorator to declare that a case function is associated with a particular target. @test_target ( int ) def case_to_test_int (): ... This is actually an alias for @case_tags(target) , that some users may find a bit more readable. @cases_generator \u00b6 @cases_generator(name_template: str, lru_cache: bool=False, **param_ranges) Decorator to declare a case function as being a cases generator. param_ranges should be a named list of parameter ranges to explore to generate the cases. The decorator will use itertools.product to create a cartesian product of the named parameter ranges, and create a case for each combination. When the case function will be called for a given combination, the corresponding parameters will be passed to the decorated function. @cases_generator ( \"test with i={i}\" , i = range ( 10 )) def case_10_times ( i ): ''' Generates 10 cases ''' ins = dict ( a = i , b = i + 1 ) outs = i + 1 , i + 2 return ins , outs , None Parameters: name_template : a name template, that will be transformed into the case name using name_template.format(**params) for each case, where params is the dictionary of parameter values for this generated case. lru_cache : a boolean (default False) indicating if the generated cases should be cached. This is identical to decorating the function with an additional @lru_cache(maxsize=n) where n is the total number of generated cases. param_ranges : named parameters and for each of them the list of values to be used to generate cases. For each combination of values (a cartesian product is made) the parameters will be passed to the underlying function so they should have names the underlying function can handle. MultipleStepsCaseData type hint \u00b6 You may wish to use this type hint instead of CaseData when your case functions may return dictionaries of given/expected_normal/expected_error. MultipleStepsCaseData = Tuple[Union[Given, Dict[Any, Given]], Union[ExpectedNormal, Dict[Any, ExpectedNormal]], Union[ExpectedError, Dict[Any, ExpectedError]]] 2 - On test functions side \u00b6 @cases_fixture \u00b6 @cases_fixture(cases=None, module=None, case_data_argname='case_data', has_tag=None, filter=None) Decorates a function so that it becomes a parametrized fixture. The fixture will be automatically parametrized with all cases listed in module module , or with all cases listed explicitly in cases . Using it with a non-None module argument is equivalent to * extracting all cases from module * then decorating your function with @pytest.fixture(params=cases) with all the cases So from pytest_cases import cases_fixture , CaseData # import the module containing the test cases import test_foo_cases @cases_fixture ( module = test_foo_cases ) def foo_fixture ( case_data : CaseData ): ... is equivalent to: import pytest from pytest_cases import get_all_cases , get_pytest_parametrize_args # import the module containing the test cases import test_foo_cases # manually list the available cases cases = get_all_cases ( module = test_foo_cases ) # transform into required arguments for pytest (applying the pytest marks if needed) marked_cases , cases_ids = get_pytest_parametrize_args ( cases ) # parametrize the fixture manually @pytest.fixture ( params = marked_cases , ids = cases_ids ) def foo_fixture ( request ): case_data = request . param # type: CaseData ... Parameters case_data_argname : the optional name of the function parameter that should receive the CaseDataGetter object. Default is case_data . Other parameters (cases, module, has_tag, filter) can be used to perform explicit listing, or filtering, of cases to include. See get_all_cases() for details about them. @cases_data \u00b6 @cases_data(cases=None, module=None, case_data_argname='case_data', has_tag=None, filter=None) Decorates a test function so as to automatically parametrize it with all cases listed in module module , or with all cases listed explicitly in cases . Using it with a non-None module argument is equivalent to extracting all cases from module then decorating your function with @pytest.mark.parametrize with all the cases So from pytest_cases import cases_data # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_foo ( case_data ): ... is equivalent to: import pytest from pytest_cases import get_all_cases , get_pytest_parametrize_args # import the module containing the test cases import test_foo_cases # manually list the available cases cases = get_all_cases ( module = test_foo_cases ) # transform into required arguments for pytest (applying the pytest marks if needed) marked_cases , cases_ids = get_pytest_parametrize_args ( cases ) # parametrize the test function manually @pytest.mark.parametrize ( 'case_data' , marked_cases , ids = str ) def test_foo ( case_data ): ... Parameters case_data_argname : the optional name of the function parameter that should receive the CaseDataGetter object. Default is case_data . Other parameters (cases, module, has_tag, filter) can be used to perform explicit listing, or filtering, of cases to include. See get_all_cases() for details about them. CaseDataGetter \u00b6 A proxy for a test case. Instances of this class are created by @cases_data or get_all_cases . It provides a single method: get(self, *args, **kwargs) -> Union[CaseData, Any] This method calls the actual underlying case function with arguments propagation, and returns the result. The case functions can use the proposed standard CaseData type hint and return outputs matching this type hint, but this is not mandatory. unfold_expected_err \u00b6 'Unfolds' the expected error expected_e to return a tuple of expected error type expected error instance error validation callable If expected_e is an exception type, returns expected_e, None, None . If expected_e is an exception instance, returns type(expected_e), expected_e, None . If expected_e is an exception validation function, returns Exception, None, expected_e . Parameters: expected_e : an ExpectedError , that is, either an exception type, an exception instance, or an exception validation function get_all_cases \u00b6 get_all_cases(cases=None, module=None, this_module_object=None, has_tag=None, filter=None) -> List[CaseDataGetter] Lists all desired cases for a given user query. This function may be convenient for debugging purposes. Parameters: cases : a single case or a hardcoded list of cases to use. Only one of cases and module should be set. module : a module or a hardcoded list of modules to use. You may use THIS_MODULE to indicate that the module is the current one. Only one of cases and module should be set. this_module_object : any variable defined in the module of interest, for example a function. It is used to find \"this module\", when module contains THIS_MODULE . has_tag : an optional tag used to filter the cases in the module . Only cases with the given tag will be selected. filter : an optional filtering function taking as an input a list of tags associated with a case, and returning a boolean indicating if the case should be selected. It will be used to filter the cases in the module . It both has_tag and filter are set, both will be applied in sequence. 3 - Pytest goodies \u00b6 @pytest_fixture_plus \u00b6 pytest_fixture_plus(scope=\"function\", autouse=False, name=None, unpack_into=None, **kwargs) Identical to @pytest.fixture decorator, except that it supports multi-parametrization with @pytest.mark.parametrize as requested in pytest#3960 . As a consequence it does not support the params and ids arguments anymore. it supports a new argument unpack_into where you can provide names for fixtures where to unpack this fixture into. As a consequence it does not support the params and ids arguments anymore. Parameters: scope : the scope for which this fixture is shared, one of \"function\" (default), \"class\", \"module\" or \"session\". autouse : if True, the fixture func is activated for all tests that can see it. If False (the default) then an explicitreference is needed to activate the fixture. name : the name of the fixture. This defaults to the name of the decorated function. Note: If a fixture is used in the same module in which it is defined, the function name of the fixture will be shadowed by the function arg that requests the fixture; one wayto resolve this is to name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') . unpack_into : an optional iterable of names, or string containing coma-separated names, for additional fixtures to create to represent parts of this fixture. See unpack_fixture for details. kwargs : other keyword arguments for @pytest.fixture unpack_fixture \u00b6 unpack_fixture(argnames, fixture) -> Tuple[<Fixture>] Creates several fixtures with names argnames from the source fixture . Created fixtures will correspond to elements unpacked from fixture in order. For example if fixture is a tuple of length 2, argnames=\"a,b\" will create two fixtures containing the first and second element respectively. The created fixtures are automatically registered into the callers' module, but you may wish to assign them to variables for convenience. In that case make sure that you use the same names, e.g. a, b = unpack_fixture('a,b', 'c') . Parameters argnames : same as @pytest.mark.parametrize argnames . fixture : a fixture name string or a fixture symbol. If a fixture symbol is provided, the created fixtures will have the same scope. If a name is provided, they will have scope='function'. Note that in practice the performance loss resulting from using function rather than a higher scope is negligible since the created fixtures' body is a one-liner. Outputs: the created fixtures. fixture_union \u00b6 fixture_union(name, fixtures, scope=\"function\", idstyle='explicit', ids=fixture_alternative_to_str, autouse=False, unpack_into=None, **kwargs) -> <Fixture> Creates a fixture that will take all values of the provided fixtures in order. That fixture is automatically registered into the callers' module, but you may wish to assign it to a variable for convenience. In that case make sure that you use the same name, e.g. a = fixture_union('a', ['b', 'c']) The style of test ids corresponding to the union alternatives can be changed with idstyle . Three values are allowed: 'explicit' (default) favors readability, 'compact' adds a small mark so that at least one sees which parameters are union parameters and which others are normal parameters, None does not change the ids. Parameters: name : the name of the fixture to create fixtures : an array-like containing fixture names and/or fixture symbols scope : the scope of the union. Since the union depends on the sub-fixtures, it should be smaller than the smallest scope of fixtures referenced. idstyle : The style of test ids corresponding to the union alternatives. One of 'explicit' (default), 'compact' , or None . unpack_into : an optional iterable of names, or string containing coma-separated names, for additional fixtures to create to represent parts of this fixture. See unpack_fixture for details. ids : as in pytest. The default value returns the correct fixture autouse : as in pytest kwargs : other pytest fixture options. They might not be supported correctly. Outputs: the new fixture. Note: you do not need to capture that output in a symbol, since the fixture is automatically registered in your module. However if you decide to do so make sure that you use the same name. param_fixtures \u00b6 param_fixtures(argnames, argvalues, autouse=False, ids=None, scope=\"function\", **kwargs) -> Tuple[<Fixture>] Creates one or several \"parameters\" fixtures - depending on the number or coma-separated names in argnames . The created fixtures are automatically registered into the callers' module, but you may wish to assign them to variables for convenience. In that case make sure that you use the same names, e.g. p, q = param_fixtures('p,q', [(0, 1), (2, 3)]) . Note that the (argnames, argvalues, ids) signature is similar to @pytest.mark.parametrize for consistency, see https://docs.pytest.org/en/latest/reference.html?highlight=pytest.param#pytest-mark-parametrize param_fixture \u00b6 param_fixture(argname, argvalues, autouse=False, ids=None, scope=\"function\", **kwargs) -> <Fixture> Identical to param_fixtures but for a single parameter name, so that you can assign its output to a single variable. @pytest_parametrize_plus \u00b6 pytest_parametrize_plus(argnames, argvalues, indirect=False, ids=None, scope=None, **kwargs) Equivalent to @pytest.mark.parametrize but also supports the fact that in argvalues one can include references to fixtures with fixture_ref(<fixture>) where can be the fixture name or fixture function. When such a fixture reference is detected in the argvalues, a new function-scope fixture will be created with a unique name, and the test function will be wrapped so as to be injected with the correct parameters. Special test ids will be created to illustrate the switching between normal parameters and fixtures.","title":"API reference"},{"location":"api_reference/#api-reference","text":"In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package.","title":"API reference"},{"location":"api_reference/#1-on-case-functions-side","text":"","title":"1 - On case functions side"},{"location":"api_reference/#casedata-type-hint","text":"A proposed standard type hint for the case functions outputs. CaseData = Tuple[Given, ExpectedNormal, ExpectedError] where Given = Any \"\"\"The input(s) for the test. It can be anything\"\"\" ExpectedNormal = Optional [ Any ] \"\"\"The expected test results in case success is expected, or None if this test should fail\"\"\" ExpectedError = Optional [ Union [ Type [ Exception ], Exception , Callable [[ Exception ], Optional [ bool ]]]] \"\"\"The expected error in case failure is expected, or None if the test should succeed. It is proposed that expected error can be defined as an exception type, an exception instance, or an exception validation function\"\"\"","title":"CaseData type hint"},{"location":"api_reference/#case_name","text":"@case_name(name: str) Decorator to override the name of a case function. The new name will be used instead of the function name, in test names. @case_name ( 'simple_case' ) def case_simple (): ...","title":"@case_name"},{"location":"api_reference/#case_tags","text":"@case_tags(*tags: Any) Decorator to tag a case function with a list of tags. These tags can then be used in the @cases_data test function decorator to filter cases within the selected module(s). Parameters: tags : a list of tags that may be used to filter the case. Tags can be anything (string, objects, types, functions...)","title":"@case_tags"},{"location":"api_reference/#test_target","text":"@test_target(target: Any) A simple decorator to declare that a case function is associated with a particular target. @test_target ( int ) def case_to_test_int (): ... This is actually an alias for @case_tags(target) , that some users may find a bit more readable.","title":"@test_target"},{"location":"api_reference/#cases_generator","text":"@cases_generator(name_template: str, lru_cache: bool=False, **param_ranges) Decorator to declare a case function as being a cases generator. param_ranges should be a named list of parameter ranges to explore to generate the cases. The decorator will use itertools.product to create a cartesian product of the named parameter ranges, and create a case for each combination. When the case function will be called for a given combination, the corresponding parameters will be passed to the decorated function. @cases_generator ( \"test with i={i}\" , i = range ( 10 )) def case_10_times ( i ): ''' Generates 10 cases ''' ins = dict ( a = i , b = i + 1 ) outs = i + 1 , i + 2 return ins , outs , None Parameters: name_template : a name template, that will be transformed into the case name using name_template.format(**params) for each case, where params is the dictionary of parameter values for this generated case. lru_cache : a boolean (default False) indicating if the generated cases should be cached. This is identical to decorating the function with an additional @lru_cache(maxsize=n) where n is the total number of generated cases. param_ranges : named parameters and for each of them the list of values to be used to generate cases. For each combination of values (a cartesian product is made) the parameters will be passed to the underlying function so they should have names the underlying function can handle.","title":"@cases_generator"},{"location":"api_reference/#multiplestepscasedata-type-hint","text":"You may wish to use this type hint instead of CaseData when your case functions may return dictionaries of given/expected_normal/expected_error. MultipleStepsCaseData = Tuple[Union[Given, Dict[Any, Given]], Union[ExpectedNormal, Dict[Any, ExpectedNormal]], Union[ExpectedError, Dict[Any, ExpectedError]]]","title":"MultipleStepsCaseData type hint"},{"location":"api_reference/#2-on-test-functions-side","text":"","title":"2 - On test functions side"},{"location":"api_reference/#cases_fixture","text":"@cases_fixture(cases=None, module=None, case_data_argname='case_data', has_tag=None, filter=None) Decorates a function so that it becomes a parametrized fixture. The fixture will be automatically parametrized with all cases listed in module module , or with all cases listed explicitly in cases . Using it with a non-None module argument is equivalent to * extracting all cases from module * then decorating your function with @pytest.fixture(params=cases) with all the cases So from pytest_cases import cases_fixture , CaseData # import the module containing the test cases import test_foo_cases @cases_fixture ( module = test_foo_cases ) def foo_fixture ( case_data : CaseData ): ... is equivalent to: import pytest from pytest_cases import get_all_cases , get_pytest_parametrize_args # import the module containing the test cases import test_foo_cases # manually list the available cases cases = get_all_cases ( module = test_foo_cases ) # transform into required arguments for pytest (applying the pytest marks if needed) marked_cases , cases_ids = get_pytest_parametrize_args ( cases ) # parametrize the fixture manually @pytest.fixture ( params = marked_cases , ids = cases_ids ) def foo_fixture ( request ): case_data = request . param # type: CaseData ... Parameters case_data_argname : the optional name of the function parameter that should receive the CaseDataGetter object. Default is case_data . Other parameters (cases, module, has_tag, filter) can be used to perform explicit listing, or filtering, of cases to include. See get_all_cases() for details about them.","title":"@cases_fixture"},{"location":"api_reference/#cases_data","text":"@cases_data(cases=None, module=None, case_data_argname='case_data', has_tag=None, filter=None) Decorates a test function so as to automatically parametrize it with all cases listed in module module , or with all cases listed explicitly in cases . Using it with a non-None module argument is equivalent to extracting all cases from module then decorating your function with @pytest.mark.parametrize with all the cases So from pytest_cases import cases_data # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_foo ( case_data ): ... is equivalent to: import pytest from pytest_cases import get_all_cases , get_pytest_parametrize_args # import the module containing the test cases import test_foo_cases # manually list the available cases cases = get_all_cases ( module = test_foo_cases ) # transform into required arguments for pytest (applying the pytest marks if needed) marked_cases , cases_ids = get_pytest_parametrize_args ( cases ) # parametrize the test function manually @pytest.mark.parametrize ( 'case_data' , marked_cases , ids = str ) def test_foo ( case_data ): ... Parameters case_data_argname : the optional name of the function parameter that should receive the CaseDataGetter object. Default is case_data . Other parameters (cases, module, has_tag, filter) can be used to perform explicit listing, or filtering, of cases to include. See get_all_cases() for details about them.","title":"@cases_data"},{"location":"api_reference/#casedatagetter","text":"A proxy for a test case. Instances of this class are created by @cases_data or get_all_cases . It provides a single method: get(self, *args, **kwargs) -> Union[CaseData, Any] This method calls the actual underlying case function with arguments propagation, and returns the result. The case functions can use the proposed standard CaseData type hint and return outputs matching this type hint, but this is not mandatory.","title":"CaseDataGetter"},{"location":"api_reference/#unfold_expected_err","text":"'Unfolds' the expected error expected_e to return a tuple of expected error type expected error instance error validation callable If expected_e is an exception type, returns expected_e, None, None . If expected_e is an exception instance, returns type(expected_e), expected_e, None . If expected_e is an exception validation function, returns Exception, None, expected_e . Parameters: expected_e : an ExpectedError , that is, either an exception type, an exception instance, or an exception validation function","title":"unfold_expected_err"},{"location":"api_reference/#get_all_cases","text":"get_all_cases(cases=None, module=None, this_module_object=None, has_tag=None, filter=None) -> List[CaseDataGetter] Lists all desired cases for a given user query. This function may be convenient for debugging purposes. Parameters: cases : a single case or a hardcoded list of cases to use. Only one of cases and module should be set. module : a module or a hardcoded list of modules to use. You may use THIS_MODULE to indicate that the module is the current one. Only one of cases and module should be set. this_module_object : any variable defined in the module of interest, for example a function. It is used to find \"this module\", when module contains THIS_MODULE . has_tag : an optional tag used to filter the cases in the module . Only cases with the given tag will be selected. filter : an optional filtering function taking as an input a list of tags associated with a case, and returning a boolean indicating if the case should be selected. It will be used to filter the cases in the module . It both has_tag and filter are set, both will be applied in sequence.","title":"get_all_cases"},{"location":"api_reference/#3-pytest-goodies","text":"","title":"3 - Pytest goodies"},{"location":"api_reference/#pytest_fixture_plus","text":"pytest_fixture_plus(scope=\"function\", autouse=False, name=None, unpack_into=None, **kwargs) Identical to @pytest.fixture decorator, except that it supports multi-parametrization with @pytest.mark.parametrize as requested in pytest#3960 . As a consequence it does not support the params and ids arguments anymore. it supports a new argument unpack_into where you can provide names for fixtures where to unpack this fixture into. As a consequence it does not support the params and ids arguments anymore. Parameters: scope : the scope for which this fixture is shared, one of \"function\" (default), \"class\", \"module\" or \"session\". autouse : if True, the fixture func is activated for all tests that can see it. If False (the default) then an explicitreference is needed to activate the fixture. name : the name of the fixture. This defaults to the name of the decorated function. Note: If a fixture is used in the same module in which it is defined, the function name of the fixture will be shadowed by the function arg that requests the fixture; one wayto resolve this is to name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') . unpack_into : an optional iterable of names, or string containing coma-separated names, for additional fixtures to create to represent parts of this fixture. See unpack_fixture for details. kwargs : other keyword arguments for @pytest.fixture","title":"@pytest_fixture_plus"},{"location":"api_reference/#unpack_fixture","text":"unpack_fixture(argnames, fixture) -> Tuple[<Fixture>] Creates several fixtures with names argnames from the source fixture . Created fixtures will correspond to elements unpacked from fixture in order. For example if fixture is a tuple of length 2, argnames=\"a,b\" will create two fixtures containing the first and second element respectively. The created fixtures are automatically registered into the callers' module, but you may wish to assign them to variables for convenience. In that case make sure that you use the same names, e.g. a, b = unpack_fixture('a,b', 'c') . Parameters argnames : same as @pytest.mark.parametrize argnames . fixture : a fixture name string or a fixture symbol. If a fixture symbol is provided, the created fixtures will have the same scope. If a name is provided, they will have scope='function'. Note that in practice the performance loss resulting from using function rather than a higher scope is negligible since the created fixtures' body is a one-liner. Outputs: the created fixtures.","title":"unpack_fixture"},{"location":"api_reference/#fixture_union","text":"fixture_union(name, fixtures, scope=\"function\", idstyle='explicit', ids=fixture_alternative_to_str, autouse=False, unpack_into=None, **kwargs) -> <Fixture> Creates a fixture that will take all values of the provided fixtures in order. That fixture is automatically registered into the callers' module, but you may wish to assign it to a variable for convenience. In that case make sure that you use the same name, e.g. a = fixture_union('a', ['b', 'c']) The style of test ids corresponding to the union alternatives can be changed with idstyle . Three values are allowed: 'explicit' (default) favors readability, 'compact' adds a small mark so that at least one sees which parameters are union parameters and which others are normal parameters, None does not change the ids. Parameters: name : the name of the fixture to create fixtures : an array-like containing fixture names and/or fixture symbols scope : the scope of the union. Since the union depends on the sub-fixtures, it should be smaller than the smallest scope of fixtures referenced. idstyle : The style of test ids corresponding to the union alternatives. One of 'explicit' (default), 'compact' , or None . unpack_into : an optional iterable of names, or string containing coma-separated names, for additional fixtures to create to represent parts of this fixture. See unpack_fixture for details. ids : as in pytest. The default value returns the correct fixture autouse : as in pytest kwargs : other pytest fixture options. They might not be supported correctly. Outputs: the new fixture. Note: you do not need to capture that output in a symbol, since the fixture is automatically registered in your module. However if you decide to do so make sure that you use the same name.","title":"fixture_union"},{"location":"api_reference/#param_fixtures","text":"param_fixtures(argnames, argvalues, autouse=False, ids=None, scope=\"function\", **kwargs) -> Tuple[<Fixture>] Creates one or several \"parameters\" fixtures - depending on the number or coma-separated names in argnames . The created fixtures are automatically registered into the callers' module, but you may wish to assign them to variables for convenience. In that case make sure that you use the same names, e.g. p, q = param_fixtures('p,q', [(0, 1), (2, 3)]) . Note that the (argnames, argvalues, ids) signature is similar to @pytest.mark.parametrize for consistency, see https://docs.pytest.org/en/latest/reference.html?highlight=pytest.param#pytest-mark-parametrize","title":"param_fixtures"},{"location":"api_reference/#param_fixture","text":"param_fixture(argname, argvalues, autouse=False, ids=None, scope=\"function\", **kwargs) -> <Fixture> Identical to param_fixtures but for a single parameter name, so that you can assign its output to a single variable.","title":"param_fixture"},{"location":"api_reference/#pytest_parametrize_plus","text":"pytest_parametrize_plus(argnames, argvalues, indirect=False, ids=None, scope=None, **kwargs) Equivalent to @pytest.mark.parametrize but also supports the fact that in argvalues one can include references to fixtures with fixture_ref(<fixture>) where can be the fixture name or fixture function. When such a fixture reference is detected in the argvalues, a new function-scope fixture will be created with a unique name, and the test function will be wrapped so as to be injected with the correct parameters. Special test ids will be created to illustrate the switching between normal parameters and fixtures.","title":"@pytest_parametrize_plus"},{"location":"changelog/","text":"Changelog \u00b6 1.11.5 - bugfix \u00b6 pytest_parametrize_plus was not working correctly with test classes, leading to fixture 'self' not found . Fixed #63 . 1.11.4 - python 2 bugfix \u00b6 Fixed issue happening with @pytest.mark.parametrize with python 2. Fixed #62 . 1.11.3 - minor improvements \u00b6 Better error message when users use THIS_MODULE in cases= instead of module= . Added __version__ package-level attribute. 1.11.2 - Increased tolerance to other plugins + bugfix \u00b6 Now when other plugins try to manipulate the fixture closure, warning messages are emitted but no error is raised. Fixed #55 . Also fixed issue #58 happening with doctest. 1.11.1 - Added six dependency explicitly \u00b6 It was missing from setup.py . 1.11.0 - fixture_ref can now be used inside tuples, leading to cross-products \u00b6 Fixes #47 . 1.10.2 - More intuitive error messages \u00b6 Now raising an explicit InvalidParamsList when pytest parametrize argvalues are incorrect. See #54 1.10.1 - Bugfix \u00b6 Fixed #52 . 1.10.0 - New feature: fixtures unpacking \u00b6 You can now unpack a fixture iterable into several individual fixtures using unpack_fixture or using @pytest_fixture_plus(unpack_into=<names>) . This is also available in union_fixture(unpack_into=<names>) . Fixed #50 and #51 . 1.9.3 - Bugfix \u00b6 Fixed issues when parametrize argnames contains a list. This fixed #49 1.9.2 - Bugfix with pytest 3.7 \u00b6 Fixed #48 . 1.9.1 - Bugfix with pytest 3.7 \u00b6 Fixed #48 . 1.9.0 - New --with-reorder commandline option \u00b6 New commandline option '--with-reorder' to change the reordering startegy currently in application. Fixes #45 . The --with-reorder \"skip\" mode was not working correctly in presence of marks, fixed it. Fixed #46 . 1.8.1 - BugFixes \u00b6 Ids should not be used when setting a NOT_USED parametrization. Fixes #43 Fixed issue with ordering and setup/teardown for higher-level scope fixtures (session and module scopes) when using union fixtures. Fixes #44 1.8.0 - Better ids for fixture unions \u00b6 New: fixture_union now accept a non- None value for ids . It also has a new idstyle argument allowing users to change the style of ids used. Finally pytest_parametrize_plus relies on this ids argument to set a more readable list of ids for the created union. Fixes #41 . Misc: Added non-regression test for fixture order. It passes already for all recent pytest versions (after 3.3). Fixes #42 1.7.0 - New @pytest_parametrize_plus allowing fixture references to be used in parameter values \u00b6 New decorator @pytest_parametrize_plus able to handle the case where a fixture_ref(<fixture_name>) is present in the parameter values list. This decorator can be applied both on test functions and fixtures (if they are decorated with @pytest_fixture_plus ). Fixes #40 Major refactoring of the \"union fixtures\" mechanism. The NOT_USED status is now correctly propagated between dependent fixtures. This should fix a few cases where user fixtures were setup/teardown while not used in the current test node. Empty fixture unions are not permitted anymore. The way unions are handled in test parametrization was redesigned. The new design is based on a two-steps approach: first build the fixture closure for each node as a tree (and not a list as in pytest ), and then apply parametrization intelligently based on this tree structure. This fixes several unintuitive behaviours that were happening with unions. Note: interestingly this also fixes pytest#5054 . 1.6.3 - Minor exception enhancement \u00b6 Improved the error message when the name template is wrong in @cases_generator . Fixes #39 . 1.6.2 - bug fixes \u00b6 fixture_union : Changed the repr of NOT_USED to pytest_cases.NOT_USED . @pytest_fixture_plus now correctly handles the NOT_USED when fixtures in the union do not contain any parameter. Fixes #38 . param_fixtures : param_fixtures now delegates to param_fixture when a single parameter name is provided. This is more consistent. Fixed #36 . param_fixture[s] now support all arguments from fixture ( scope and autouse in particular). 1.6.1 - @pytest_fixture_plus improvement to handle NOT_USED cases \u00b6 Fixed issue where fixtures get called with NOT_USED as a parameter when using a fixture_union . This issue is actually only fixed in @pytest_fixture_plus , if you use @pytest.fixture you have to handle it manually. Fixes #37 . 1.6.0 - fixture_union and param_fixture[s] bugfix \u00b6 New fixture_union method to create a fixture that is the union/combination of other fixtures. This is an attempt to solve this pytest proposal . Also, param_fixture and param_fixtures can now be used without necessarily storing the return value into a variable: they will automatically register the created fixtures in the calling module. Finally, fixed a bug with param_fixtures when called to create a fixture for a single parameter. 1.5.1 - param_fixtures bugfix \u00b6 Fixed param_fixtures issue: all parameter values were identical to the last parameter of the tuple. Fixes #32 . 1.5.0 - new helpers param_fixture and param_fixtures \u00b6 Following Sup3rGeo 's proposal, introduced two helper methods to create simple \"parameter fixtures\". Fixes #31 . 1.4.2 - parametrized @pytest_fixture_plus minor bug fix \u00b6 @pytest_fixture_plus now correctly honors parameter id and marks overriden at single parameter level using pytest.param . Fixed #30 . 1.4.1 - parametrized @pytest_fixture_plus minor bug fix \u00b6 Fixed @pytest_fixture_plus in case it is used with parametrize and one parameter is itself customized using pytest.param . Fixed #29 . 1.4.0 - @pytest_fixture_plus major improvement \u00b6 Major improvement of @pytest_fixture_plus : instead of generating fixtures, it now correctly parametrizes the fixture. Skip/fail Marks are correctly copied too. Fixes #28 . pytest_fixture_plus does not accept the params and ids arguments any more, it only relies on parametrization marks. 1.3.3 - parametrized @pytest_fixture_plus Bugfix \u00b6 Fixed minor bug with parametrized @pytest_fixture_plus : spaces are now correctly removed when multiple parameter names are provided in the same parametrize call. Fixes #27 . 1.3.2 - parametrized @pytest_fixture_plus Bugfix \u00b6 Fixed bug with @pytest_fixture_plus when used in parametrized mode. Fixes #26 . Thanks Sup3rGeo ! 1.3.1 - Minor dependency change \u00b6 Now using decopatch to create the decorators. 1.3.0 - More flexible case generators names + Minor dependency change \u00b6 Cases generators can now support explicit name lists, and name generator callables, in addition to the name template strings. Fixed #24 . Dependency to decorator has been dropped and replaced with makefun . Fixed #25 . 1.2.2 - fixed bug with marks on cases with pytest 3.3 \u00b6 Marks on cases are now also working with pytest 3.3. Fixed #23 . Ids for marked tests are now better managed. A new function get_pytest_parametrize_args is now used to transform the list of cases obtained by get_all_cases(module) , into the list of marked cases and ids required by @pytest.mark.parametrize . The doc has been updated to explain this for advanced users wishing to perform this step manually. 1.2.1 - fixed id of test cases with marks \u00b6 Id of test cases with marks was appearing as ParameterSet . Fixed it. 1.2.0 - @pytest.mark can be used on cases + @pytest_fixture_plus parametrization order bugfix \u00b6 Pytest marks such as @pytest.mark.skipif can now be used on case functions. As a consequence, get_all_cases is now the recommended function to use instead of extract_cases_from_module to perform manual collection. Indeed get_all_cases correctly prepares the resulting parameters list so that pytest sees the marks. Fixed #21 . Fixed parametrization order when @pytest_fixture_plus is used with several @pytest.mark.parametrize . Fixed #22 . 1.1.1 - Improved generated fixture names for @pytest_fixture_plus \u00b6 When @pytest_fixture_plus is used on a function marked as parametrized, some fixtures are generated (one for each parameter). Generated fixture names now follow the pattern <fixturename>__<paramname> . Fixed #20 . 1.1.0 - New @pytest_fixture_plus \u00b6 New decorator @pytest_fixture_plus allows to use several @pytest.mark.parametrize on a fixture. Therefore one can use multiple @cases_data decorators, too. Fixes #19 . Note: this is a temporary feature, that will be removed if/when pytest supports it . 1.0.0 - @cases_fixture + pytest 2.x support \u00b6 Pytest 2.x is now supported. Fixes #14 . New feature: @cases_fixture ! Now you can put your cases data retrieval in a fixture so that its duration does not enter into the test duration. This is particularly interesting if you use pytest-harvest to create benchmarks: you probably do not want the case data retrieval/parsing to be counted in the test duration, especially if you use caching on the case function to accelerate subsequent retrievals. Fixes #15 . 0.10.1 - minor encoding issue in setup.py \u00b6 0.10.0 - support for python 2 \u00b6 Python 2 is now supported. Fixed #3 . Note: CaseData , Given , ExpectedNormal , ExpectedError , and MultipleStepsCaseData type hints is not created in python 2 and python<3.5 0.9.1 - pytest-steps is now an independent project \u00b6 Light refactoring: some internal function names are now private, and there are now two submodules. pytest-steps is now an independent project. Examples in the documentation have been updated New documentation page: API reference 0.8.0 - Filtering can now be done using a callable. \u00b6 @cases_data : the filter argument now contains a filtering function. WARNING: the previous behaviour is still available but has been renamed has_tag . Fixes #8 . 0.7.0 - Hardcoded cases selection, and multi-module selection \u00b6 @cases_data has a new parameters cases that can be used to hardcode a case or a list of cases. Its module parameter can also now take a list of modules 0.6.0 - Case parameters and better test suites \u00b6 get_for is deprecated: it was too specific to a given case data format. MultipleStepsCaseData was fixed to also support multiple inputs. Case functions can now have parameters (even case generators). This is particularly useful for test suites. Fixes #9 . 0.5.0 - support for test suites \u00b6 test functions can now be decorated with @test_steps to easily define a test suite with several steps. This fixes #7 . 0.4.0 - support for data caching with lru_cache \u00b6 cases can now be decorated with @lru_cache . @cases_generator also provides a lru_cache parameter to enable caching. Fixes #6 . 0.3.0 - case generators \u00b6 New decorator @cases_generator to define case generators. Fixes #1 . Also, removed unused functions is_expected_error_instance and assert_exception_equal 0.2.0 - THIS_MODULE constant + Tagging/Filtering + doc \u00b6 New constant THIS_MODULE so that cases and test functions can coexist in the same file. This fixes #5 . Added @test_target and @case_tags decorators for case functions, and added filter parameter in @cases_data . This allows users to : tag a case function with any item (and in particular with the reference to the function it relates to), and to filter the case functions used by a test function according to a particular tag. This fixes #4 . Improved documentation 0.1.0 - First public version \u00b6 Initial fork from private repo","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#1115-bugfix","text":"pytest_parametrize_plus was not working correctly with test classes, leading to fixture 'self' not found . Fixed #63 .","title":"1.11.5 - bugfix"},{"location":"changelog/#1114-python-2-bugfix","text":"Fixed issue happening with @pytest.mark.parametrize with python 2. Fixed #62 .","title":"1.11.4 - python 2 bugfix"},{"location":"changelog/#1113-minor-improvements","text":"Better error message when users use THIS_MODULE in cases= instead of module= . Added __version__ package-level attribute.","title":"1.11.3 - minor improvements"},{"location":"changelog/#1112-increased-tolerance-to-other-plugins-bugfix","text":"Now when other plugins try to manipulate the fixture closure, warning messages are emitted but no error is raised. Fixed #55 . Also fixed issue #58 happening with doctest.","title":"1.11.2 - Increased tolerance to other plugins + bugfix"},{"location":"changelog/#1111-added-six-dependency-explicitly","text":"It was missing from setup.py .","title":"1.11.1 - Added six dependency explicitly"},{"location":"changelog/#1110-fixture_ref-can-now-be-used-inside-tuples-leading-to-cross-products","text":"Fixes #47 .","title":"1.11.0 - fixture_ref can now be used inside tuples, leading to cross-products"},{"location":"changelog/#1102-more-intuitive-error-messages","text":"Now raising an explicit InvalidParamsList when pytest parametrize argvalues are incorrect. See #54","title":"1.10.2 - More intuitive error messages"},{"location":"changelog/#1101-bugfix","text":"Fixed #52 .","title":"1.10.1 - Bugfix"},{"location":"changelog/#1100-new-feature-fixtures-unpacking","text":"You can now unpack a fixture iterable into several individual fixtures using unpack_fixture or using @pytest_fixture_plus(unpack_into=<names>) . This is also available in union_fixture(unpack_into=<names>) . Fixed #50 and #51 .","title":"1.10.0 - New feature: fixtures unpacking"},{"location":"changelog/#193-bugfix","text":"Fixed issues when parametrize argnames contains a list. This fixed #49","title":"1.9.3 - Bugfix"},{"location":"changelog/#192-bugfix-with-pytest-37","text":"Fixed #48 .","title":"1.9.2 - Bugfix with pytest 3.7"},{"location":"changelog/#191-bugfix-with-pytest-37","text":"Fixed #48 .","title":"1.9.1 - Bugfix with pytest 3.7"},{"location":"changelog/#190-new-with-reorder-commandline-option","text":"New commandline option '--with-reorder' to change the reordering startegy currently in application. Fixes #45 . The --with-reorder \"skip\" mode was not working correctly in presence of marks, fixed it. Fixed #46 .","title":"1.9.0 - New --with-reorder commandline option"},{"location":"changelog/#181-bugfixes","text":"Ids should not be used when setting a NOT_USED parametrization. Fixes #43 Fixed issue with ordering and setup/teardown for higher-level scope fixtures (session and module scopes) when using union fixtures. Fixes #44","title":"1.8.1 - BugFixes"},{"location":"changelog/#180-better-ids-for-fixture-unions","text":"New: fixture_union now accept a non- None value for ids . It also has a new idstyle argument allowing users to change the style of ids used. Finally pytest_parametrize_plus relies on this ids argument to set a more readable list of ids for the created union. Fixes #41 . Misc: Added non-regression test for fixture order. It passes already for all recent pytest versions (after 3.3). Fixes #42","title":"1.8.0 - Better ids for fixture unions"},{"location":"changelog/#170-new-pytest_parametrize_plus-allowing-fixture-references-to-be-used-in-parameter-values","text":"New decorator @pytest_parametrize_plus able to handle the case where a fixture_ref(<fixture_name>) is present in the parameter values list. This decorator can be applied both on test functions and fixtures (if they are decorated with @pytest_fixture_plus ). Fixes #40 Major refactoring of the \"union fixtures\" mechanism. The NOT_USED status is now correctly propagated between dependent fixtures. This should fix a few cases where user fixtures were setup/teardown while not used in the current test node. Empty fixture unions are not permitted anymore. The way unions are handled in test parametrization was redesigned. The new design is based on a two-steps approach: first build the fixture closure for each node as a tree (and not a list as in pytest ), and then apply parametrization intelligently based on this tree structure. This fixes several unintuitive behaviours that were happening with unions. Note: interestingly this also fixes pytest#5054 .","title":"1.7.0 - New @pytest_parametrize_plus allowing fixture references to be used in parameter values"},{"location":"changelog/#163-minor-exception-enhancement","text":"Improved the error message when the name template is wrong in @cases_generator . Fixes #39 .","title":"1.6.3 - Minor exception enhancement"},{"location":"changelog/#162-bug-fixes","text":"fixture_union : Changed the repr of NOT_USED to pytest_cases.NOT_USED . @pytest_fixture_plus now correctly handles the NOT_USED when fixtures in the union do not contain any parameter. Fixes #38 . param_fixtures : param_fixtures now delegates to param_fixture when a single parameter name is provided. This is more consistent. Fixed #36 . param_fixture[s] now support all arguments from fixture ( scope and autouse in particular).","title":"1.6.2 - bug fixes"},{"location":"changelog/#161-pytest_fixture_plus-improvement-to-handle-not_used-cases","text":"Fixed issue where fixtures get called with NOT_USED as a parameter when using a fixture_union . This issue is actually only fixed in @pytest_fixture_plus , if you use @pytest.fixture you have to handle it manually. Fixes #37 .","title":"1.6.1 - @pytest_fixture_plus improvement to handle NOT_USED cases"},{"location":"changelog/#160-fixture_union-and-param_fixtures-bugfix","text":"New fixture_union method to create a fixture that is the union/combination of other fixtures. This is an attempt to solve this pytest proposal . Also, param_fixture and param_fixtures can now be used without necessarily storing the return value into a variable: they will automatically register the created fixtures in the calling module. Finally, fixed a bug with param_fixtures when called to create a fixture for a single parameter.","title":"1.6.0 - fixture_union and param_fixture[s] bugfix"},{"location":"changelog/#151-param_fixtures-bugfix","text":"Fixed param_fixtures issue: all parameter values were identical to the last parameter of the tuple. Fixes #32 .","title":"1.5.1 - param_fixtures bugfix"},{"location":"changelog/#150-new-helpers-param_fixture-and-param_fixtures","text":"Following Sup3rGeo 's proposal, introduced two helper methods to create simple \"parameter fixtures\". Fixes #31 .","title":"1.5.0 - new helpers param_fixture and param_fixtures"},{"location":"changelog/#142-parametrized-pytest_fixture_plus-minor-bug-fix","text":"@pytest_fixture_plus now correctly honors parameter id and marks overriden at single parameter level using pytest.param . Fixed #30 .","title":"1.4.2 - parametrized @pytest_fixture_plus minor bug fix"},{"location":"changelog/#141-parametrized-pytest_fixture_plus-minor-bug-fix","text":"Fixed @pytest_fixture_plus in case it is used with parametrize and one parameter is itself customized using pytest.param . Fixed #29 .","title":"1.4.1 - parametrized @pytest_fixture_plus minor bug fix"},{"location":"changelog/#140-pytest_fixture_plus-major-improvement","text":"Major improvement of @pytest_fixture_plus : instead of generating fixtures, it now correctly parametrizes the fixture. Skip/fail Marks are correctly copied too. Fixes #28 . pytest_fixture_plus does not accept the params and ids arguments any more, it only relies on parametrization marks.","title":"1.4.0 - @pytest_fixture_plus major improvement"},{"location":"changelog/#133-parametrized-pytest_fixture_plus-bugfix","text":"Fixed minor bug with parametrized @pytest_fixture_plus : spaces are now correctly removed when multiple parameter names are provided in the same parametrize call. Fixes #27 .","title":"1.3.3 - parametrized @pytest_fixture_plus Bugfix"},{"location":"changelog/#132-parametrized-pytest_fixture_plus-bugfix","text":"Fixed bug with @pytest_fixture_plus when used in parametrized mode. Fixes #26 . Thanks Sup3rGeo !","title":"1.3.2 - parametrized @pytest_fixture_plus Bugfix"},{"location":"changelog/#131-minor-dependency-change","text":"Now using decopatch to create the decorators.","title":"1.3.1 - Minor dependency change"},{"location":"changelog/#130-more-flexible-case-generators-names-minor-dependency-change","text":"Cases generators can now support explicit name lists, and name generator callables, in addition to the name template strings. Fixed #24 . Dependency to decorator has been dropped and replaced with makefun . Fixed #25 .","title":"1.3.0 - More flexible case generators names + Minor dependency change"},{"location":"changelog/#122-fixed-bug-with-marks-on-cases-with-pytest-33","text":"Marks on cases are now also working with pytest 3.3. Fixed #23 . Ids for marked tests are now better managed. A new function get_pytest_parametrize_args is now used to transform the list of cases obtained by get_all_cases(module) , into the list of marked cases and ids required by @pytest.mark.parametrize . The doc has been updated to explain this for advanced users wishing to perform this step manually.","title":"1.2.2 - fixed bug with marks on cases with pytest 3.3"},{"location":"changelog/#121-fixed-id-of-test-cases-with-marks","text":"Id of test cases with marks was appearing as ParameterSet . Fixed it.","title":"1.2.1 - fixed id of test cases with marks"},{"location":"changelog/#120-pytestmark-can-be-used-on-cases-pytest_fixture_plus-parametrization-order-bugfix","text":"Pytest marks such as @pytest.mark.skipif can now be used on case functions. As a consequence, get_all_cases is now the recommended function to use instead of extract_cases_from_module to perform manual collection. Indeed get_all_cases correctly prepares the resulting parameters list so that pytest sees the marks. Fixed #21 . Fixed parametrization order when @pytest_fixture_plus is used with several @pytest.mark.parametrize . Fixed #22 .","title":"1.2.0 - @pytest.mark can be used on cases + @pytest_fixture_plus parametrization order bugfix"},{"location":"changelog/#111-improved-generated-fixture-names-for-pytest_fixture_plus","text":"When @pytest_fixture_plus is used on a function marked as parametrized, some fixtures are generated (one for each parameter). Generated fixture names now follow the pattern <fixturename>__<paramname> . Fixed #20 .","title":"1.1.1 - Improved generated fixture names for @pytest_fixture_plus"},{"location":"changelog/#110-new-pytest_fixture_plus","text":"New decorator @pytest_fixture_plus allows to use several @pytest.mark.parametrize on a fixture. Therefore one can use multiple @cases_data decorators, too. Fixes #19 . Note: this is a temporary feature, that will be removed if/when pytest supports it .","title":"1.1.0 - New @pytest_fixture_plus"},{"location":"changelog/#100-cases_fixture-pytest-2x-support","text":"Pytest 2.x is now supported. Fixes #14 . New feature: @cases_fixture ! Now you can put your cases data retrieval in a fixture so that its duration does not enter into the test duration. This is particularly interesting if you use pytest-harvest to create benchmarks: you probably do not want the case data retrieval/parsing to be counted in the test duration, especially if you use caching on the case function to accelerate subsequent retrievals. Fixes #15 .","title":"1.0.0 - @cases_fixture + pytest 2.x support"},{"location":"changelog/#0101-minor-encoding-issue-in-setuppy","text":"","title":"0.10.1 - minor encoding issue in setup.py"},{"location":"changelog/#0100-support-for-python-2","text":"Python 2 is now supported. Fixed #3 . Note: CaseData , Given , ExpectedNormal , ExpectedError , and MultipleStepsCaseData type hints is not created in python 2 and python<3.5","title":"0.10.0 - support for python 2"},{"location":"changelog/#091-pytest-steps-is-now-an-independent-project","text":"Light refactoring: some internal function names are now private, and there are now two submodules. pytest-steps is now an independent project. Examples in the documentation have been updated New documentation page: API reference","title":"0.9.1 - pytest-steps is now an independent project"},{"location":"changelog/#080-filtering-can-now-be-done-using-a-callable","text":"@cases_data : the filter argument now contains a filtering function. WARNING: the previous behaviour is still available but has been renamed has_tag . Fixes #8 .","title":"0.8.0 - Filtering can now be done using a callable."},{"location":"changelog/#070-hardcoded-cases-selection-and-multi-module-selection","text":"@cases_data has a new parameters cases that can be used to hardcode a case or a list of cases. Its module parameter can also now take a list of modules","title":"0.7.0 - Hardcoded cases selection, and multi-module selection"},{"location":"changelog/#060-case-parameters-and-better-test-suites","text":"get_for is deprecated: it was too specific to a given case data format. MultipleStepsCaseData was fixed to also support multiple inputs. Case functions can now have parameters (even case generators). This is particularly useful for test suites. Fixes #9 .","title":"0.6.0 - Case parameters and better test suites"},{"location":"changelog/#050-support-for-test-suites","text":"test functions can now be decorated with @test_steps to easily define a test suite with several steps. This fixes #7 .","title":"0.5.0 - support for test suites"},{"location":"changelog/#040-support-for-data-caching-with-lru_cache","text":"cases can now be decorated with @lru_cache . @cases_generator also provides a lru_cache parameter to enable caching. Fixes #6 .","title":"0.4.0 - support for data caching with lru_cache"},{"location":"changelog/#030-case-generators","text":"New decorator @cases_generator to define case generators. Fixes #1 . Also, removed unused functions is_expected_error_instance and assert_exception_equal","title":"0.3.0 - case generators"},{"location":"changelog/#020-this_module-constant-taggingfiltering-doc","text":"New constant THIS_MODULE so that cases and test functions can coexist in the same file. This fixes #5 . Added @test_target and @case_tags decorators for case functions, and added filter parameter in @cases_data . This allows users to : tag a case function with any item (and in particular with the reference to the function it relates to), and to filter the case functions used by a test function according to a particular tag. This fixes #4 . Improved documentation","title":"0.2.0 - THIS_MODULE constant + Tagging/Filtering + doc"},{"location":"changelog/#010-first-public-version","text":"Initial fork from private repo","title":"0.1.0 - First public version"},{"location":"long_description/","text":"pytest-cases \u00b6 Separate test code from test cases in pytest . The documentation for users is available here: https://smarie.github.io/python-pytest-cases/ A readme for developers is available here: https://github.com/smarie/python-pytest-cases","title":"pytest-cases"},{"location":"long_description/#pytest-cases","text":"Separate test code from test cases in pytest . The documentation for users is available here: https://smarie.github.io/python-pytest-cases/ A readme for developers is available here: https://github.com/smarie/python-pytest-cases","title":"pytest-cases"},{"location":"usage/","text":"Usage \u00b6 You have seen in the main page a small example to understand the concepts. pytest_cases provides a few additional goodies to go further. Basic : the \"must read\" to get started Intermediate Advanced You can also wish to look at the API reference in addition.","title":"Overview"},{"location":"usage/#usage","text":"You have seen in the main page a small example to understand the concepts. pytest_cases provides a few additional goodies to go further. Basic : the \"must read\" to get started Intermediate Advanced You can also wish to look at the API reference in addition.","title":"Usage"},{"location":"usage/advanced/","text":"Advanced usage \u00b6 Case arguments \u00b6 Case functions can have arguments. This makes it very easy to combine test cases with more elaborate pytest concepts (fixtures, parameters): import pytest from pytest_cases import CaseData , cases_data , CaseDataGetter , THIS_MODULE def case_simple ( version : str ) -> CaseData : print ( \"using version \" + version ) ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None def case_simple2 ( version : str ) -> CaseData : print ( \"using version \" + version ) ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None # the order of the loops will be [for version] > [for case] @cases_data ( module = THIS_MODULE ) @pytest.mark.parametrize ( \"version\" , [ \"1.0.0\" , \"2.0.0\" ]) def test_with_parameters ( case_data : CaseDataGetter , version ): # 1- Grab the test case data with the parameter i , expected_o , expected_e = case_data . get ( version ) # 2- Use it as usual... # ... This also works with case generators: simply add the argument(s) to the function signature, without declaring them in the @cases_generator decorator. @cases_generator ( \"gen case i={i}, j={j}\" , i = range ( 2 ), j = range ( 2 )) def case_gen ( version : str , i : int , j : int ) -> CaseData : print ( \"using version \" + version ) ins = dict ( a = i , b = j ) outs = i + 1 , j + 1 return ins , outs , None Reusing cases in several Tests \u00b6 You might wish to use the same test cases in several test functions. This works out of the box: simply refer to the same test case module in the @case_data decorator of several test functions, and you're set! import pytest from pytest_cases import cases_data , CaseDataGetter # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_1 ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it # ... @cases_data ( module = test_foo_cases ) def test_2 ( case_data : CaseDataGetter ): \"\"\" Another test that uses exactly the same test case data than test_1 \"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it # ... With variants \u00b6 If the tests use the same shared cases but with small differences, you may wish to add arguments to your case functions. Caching \u00b6 After starting to reuse cases in several test functions, you might end-up thinking \"why do I have to spend the data parsing/generation time several times ? It is the same case.\" . You can solve this issue by using a cache. For simple cases you can simply decorate your case function with @lru_cache(maxsize=1) since simple case functions do not have arguments: from functools import lru_cache @lru_cache ( maxsize = 1 ) def case_a (): # ... (as usual) For case generators you can also use @lru_cache(maxsize=x) , but you will have to set the max size according to the number of generated cases (or None to allow auto-grow). This can be automated: simply use the lru_cache=True parameter and pytest-cases will do it for you: from pytest_cases import CaseData , cases_data , CaseDataGetter , THIS_MODULE , \\ cases_generator # ----------------------CASES-------------------------- # case generator with caching enabled @cases_generator ( \"case {i}\" , i = range ( 3 ), lru_cache = True ) def case_gen ( i ) -> CaseData : print ( \"generating case \" + str ( i )) ins = i outs , err = None , None return ins , outs , err # ----------------------TESTS-------------------------- @cases_data ( module = THIS_MODULE ) def test_a ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it ... @cases_data ( module = THIS_MODULE ) def test_b ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it ... yields: ============================= test session starts ============================= ... collecting ... collected 6 items test_memoize_generators.py::test_a [ case 0 ] PASSED [ 16 % ] generating case 0 0 test_memoize_generators.py::test_a [ case 1 ] PASSED [ 33 % ] generating case 1 1 test_memoize_generators.py::test_a [ case 2 ] PASSED [ 50 % ] generating case 2 2 test_memoize_generators.py::test_b [ case 0 ] PASSED [ 66 % ] 0 test_memoize_generators.py::test_b [ case 1 ] PASSED [ 83 % ] 1 test_memoize_generators.py::test_b [ case 2 ] PASSED [ 100 % ] 2 ========================== 6 passed in 0 .16 seconds =========================== You can see that the second time each case is needed, the cached value is used instead of executing the case generation function again. See doc on lru_cache for implementation details. WARNING if you use case arguments , do not forget to take the additional parameter values into account to estimate the total cache size. Note that the lru_cache= option of @cases_generator is not intelligent enough to handle additional arguments: do not use it, and instead manually apply the @lru_cache decorator. Incremental tests with pytest-steps \u00b6 Sometimes you wish to execute a series of test steps on the same dataset, and then to move to another one. There are many ways to do this with pytest but some of them are not easy to blend with the notion of 'cases' in an intuitive manner. pytest-cases is compliant with pytest-steps : you can easily create incremental tests and throw your cases on them. This tutorial assumes that you are already familiar with pytest-steps . 1- If steps can run with the same data \u00b6 If all of the test steps require the same data to execute, it is straightforward, both in parametrizer mode (shown below) or in the new pytest steps generator mode (not shown): from pytest_cases import cases_data , CaseDataGetter , THIS_MODULE , CaseData from pytest_steps import test_steps # -------- test cases def case_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) return ins , None , None def case_simple2 () -> CaseData : ins = dict ( a =- 1 , b = 2 ) return ins , None , None # ------- test steps def step_a ( steps_data , ins , expected_o , expected_e ): \"\"\" Step a of the test \"\"\" # Use the three items as usual ... # You can also store intermediate results in steps_data def step_b ( steps_data , ins , expected_o , expected_e ): \"\"\" Step b of the test \"\"\" # Use the three items as usual ... # You can also retrieve intermediate results from steps_data # ------- test suite @test_steps ( step_a , step_b ) @cases_data ( module = THIS_MODULE ) def test_suite ( test_step , case_data : CaseDataGetter , steps_data ): # Get the data for this step ins , expected_o , expected_e = case_data . get () # Execute the step test_step ( steps_data , ins , expected_o , expected_e ) This yields: ============================= test session starts ============================= ... test_p.py::test_suite [ case_simple-step_a ] PASSED [ 25 % ]{ 'a' : 1 , 'b' : 2 } test_p.py::test_suite [ case_simple-step_b ] PASSED [ 50 % ]{ 'a' : 1 , 'b' : 2 } test_p.py::test_suite [ case_simple2-step_a ] PASSED [ 75 % ]{ 'a' : -1, 'b' : 2 } test_p.py::test_suite [ case_simple2-step_b ] PASSED [ 100 % ]{ 'a' : -1, 'b' : 2 } ========================== 4 passed in 0 .13 seconds =========================== You see that for each case data, all steps are executed in order. If you use an IDE it will appear in this intuitive order too: case_simple - step_a - step_b case_simple2 - step_a - step_b If for some reason you wish to invert the order (executing all cases on step a then all cases on step b etc.) simply invert the order of decorators and it will work ( pytest is great!). This is not recommended though, as it is a lot less intuitive. Of course you might want to enable caching so that the cases will be read only once, and not once for each test step. 2- If steps require different data (A: dicts) \u00b6 In real-world usage, each step will probably have different expected output or errors for the same case - except if the steps are very similar. The steps may even need slightly different input, for example the same dataset but in two different formats. This is actually quite straightforward: simply adapt your custom case data format definition! Since pytest-cases does not impose any format for your case functions outputs, you can decide that your case functions return lists, dictionaries, etc. For example you can choose the format proposed by the MultipleStepsCaseData type hint, where each item in the returned inputs/outputs/errors tuple can either be a single element, or a dictionary of name -> element. This allows your case functions to return alternate contents depending on the test step being executed. The example below shows a test suite where the inputs of the steps are the same, but the outputs and expected errors are different. Note that once again the example relies on the legacy \"parametrizer\" mode of pytest-steps, but it would be similar with the new \"generator\" mode. from pytest_cases import cases_data , CaseDataGetter , THIS_MODULE , \\ MultipleStepsCaseData from pytest_steps import test_steps # -------- test cases def case_simple () -> MultipleStepsCaseData : # common input ins = dict ( a = 1 , b = 2 ) # one expected output for each step outs_for_a = 2 , 3 outs_for_b = 5 , 4 outs = dict ( step_check_a = outs_for_a , step_check_b = outs_for_b ) return ins , outs , None def case_simple2 () -> MultipleStepsCaseData : # common input ins = dict ( a =- 1 , b = 2 ) # one expected output for each step outs_for_a = 2 , 3 outs_for_b = 5 , 4 outs = dict ( step_check_a = outs_for_a , step_check_b = outs_for_b ) return ins , outs , None # ------- test steps def step_check_a ( steps_data , ins , expected_o , expected_e ): \"\"\" Step a of the test \"\"\" # Use the three items as usual ... # You can also store intermediate results in steps_data def step_check_b ( steps_data , ins , expected_o , expected_e ): \"\"\" Step b of the test \"\"\" # Use the three items as usual ... # You can also retrieve intermediate results from steps_data # ------- test suite @test_steps ( step_check_a , step_check_b ) @cases_data ( module = THIS_MODULE ) def test_suite ( test_step , case_data : CaseDataGetter , steps_data ): # Get the case data for all steps (sad...) ins , expected_o , expected_e = case_data . get () # Filter it, based on the step name key = test_step . __name__ expected_o = None if expected_o is None else expected_o [ key ] expected_e = None if expected_e is None else expected_e [ key ] # Execute the step test_step ( steps_data , ins , expected_o , expected_e ) There are two main differences with the previous example: in the case_simple and case_simple2 case functions, we choose to provide the expected output and expected error as dictionaries indexed by the test step name when they are non- None . We also choose that the input is the same for all steps, but we could have done otherwise - using a dictionary with step name keys as well. in the final test_suite we use the test step name dictionary key to get the case data contents for a given step . Once again you might want to enable caching in order for the cases to be read only once, and not once for each test step. 3- If steps require different data (B: arguments, recommended) \u00b6 The above example might seem a bit disappointing as it breaks the philosophy of doing each data access only when it is needed . Indeed everytime a single step is run it actually gets the data for all steps and then has to do some filtering. The style suggested below, making use of case arguments , is probably better: from pytest_cases import cases_data , CaseDataGetter , THIS_MODULE , CaseData from pytest_steps import test_steps # -------- test cases def case_simple ( step_name : str ) -> CaseData : # reuse the same input whatever the step ins = dict ( a = 1 , b = 2 ) # adapt the expected output to the current step if step_name == 'step_check_a' : outs = 2 , 3 elif step_name == 'step_check_b' : outs = 5 , 4 return ins , outs , None def case_simple2 ( step_name : str ) -> CaseData : # reuse the same input whatever the step ins = dict ( a =- 1 , b = 2 ) # adapt the expected output to the current step if step_name == 'step_check_a' : outs = 0 , 3 elif step_name == 'step_check_b' : outs = 1 , 4 return ins , outs , None # ------- test steps def step_check_a ( steps_data , ins , expected_o , expected_e ): \"\"\" Step a of the test \"\"\" # Use the three items as usual ... # You can also store intermediate results in steps_data def step_check_b ( steps_data , ins , expected_o , expected_e ): \"\"\" Step b of the test \"\"\" # Use the three items as usual ... # You can also retrieve intermediate results from steps_data # ------- test suite @test_steps ( step_check_a , step_check_b ) @cases_data ( module = THIS_MODULE ) def test_suite ( test_step , case_data : CaseDataGetter , steps_data ): # Get the data for this particular case ins , expected_o , expected_e = case_data . get ( test_step . __name__ ) # Execute the step test_step ( steps_data , ins , expected_o , expected_e ) Notice that now the test step name is a parameter of the case function . So for each step, only the data relevant to this step is retrieved. Once again you might want to enable caching in order for the cases to be read only once, and not once for each test step. However since the case functions now have arguments, you should not use @lru_cache() directly on the case function but you should put it in a separate subfunction: from pytest_cases import CaseData from functools import lru_cache @lru_cache () def input_for_case_simple (): return dict ( a = 1 , b = 2 ) def case_simple ( step_name : str ) -> CaseData : # reuse the same input whatever the step ins = input_for_case_simple () # adapt the expected output to the current step if step_name == 'step_check_a' : outs = 2 , 3 elif step_name == 'step_check_b' : outs = 5 , 4 return ins , outs , None That way, input_for_case_simple will be cached across the steps. See caching for details. Advanced Pytest: Manual parametrization \u00b6 The @cases_data decorator is just syntactic sugar for the following two-steps process, that you may wish to rely on for advanced pytest usages: import pytest from pytest_cases import get_all_cases , get_pytest_parametrize_args # import the module containing the test cases import test_foo_cases # manually list the available cases cases = get_all_cases ( module = test_foo_cases ) # transform into required arguments for pytest (applying the pytest marks if needed) marked_cases , cases_ids = get_pytest_parametrize_args ( cases ) # parametrize the test function manually @pytest.mark.parametrize ( 'case_data' , marked_cases , ids = cases_ids ) def test_with_cases_decorated ( case_data ): ...","title":"Advanced"},{"location":"usage/advanced/#advanced-usage","text":"","title":"Advanced usage"},{"location":"usage/advanced/#case-arguments","text":"Case functions can have arguments. This makes it very easy to combine test cases with more elaborate pytest concepts (fixtures, parameters): import pytest from pytest_cases import CaseData , cases_data , CaseDataGetter , THIS_MODULE def case_simple ( version : str ) -> CaseData : print ( \"using version \" + version ) ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None def case_simple2 ( version : str ) -> CaseData : print ( \"using version \" + version ) ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None # the order of the loops will be [for version] > [for case] @cases_data ( module = THIS_MODULE ) @pytest.mark.parametrize ( \"version\" , [ \"1.0.0\" , \"2.0.0\" ]) def test_with_parameters ( case_data : CaseDataGetter , version ): # 1- Grab the test case data with the parameter i , expected_o , expected_e = case_data . get ( version ) # 2- Use it as usual... # ... This also works with case generators: simply add the argument(s) to the function signature, without declaring them in the @cases_generator decorator. @cases_generator ( \"gen case i={i}, j={j}\" , i = range ( 2 ), j = range ( 2 )) def case_gen ( version : str , i : int , j : int ) -> CaseData : print ( \"using version \" + version ) ins = dict ( a = i , b = j ) outs = i + 1 , j + 1 return ins , outs , None","title":"Case arguments"},{"location":"usage/advanced/#reusing-cases-in-several-tests","text":"You might wish to use the same test cases in several test functions. This works out of the box: simply refer to the same test case module in the @case_data decorator of several test functions, and you're set! import pytest from pytest_cases import cases_data , CaseDataGetter # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_1 ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it # ... @cases_data ( module = test_foo_cases ) def test_2 ( case_data : CaseDataGetter ): \"\"\" Another test that uses exactly the same test case data than test_1 \"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it # ...","title":"Reusing cases in several Tests"},{"location":"usage/advanced/#with-variants","text":"If the tests use the same shared cases but with small differences, you may wish to add arguments to your case functions.","title":"With variants"},{"location":"usage/advanced/#caching","text":"After starting to reuse cases in several test functions, you might end-up thinking \"why do I have to spend the data parsing/generation time several times ? It is the same case.\" . You can solve this issue by using a cache. For simple cases you can simply decorate your case function with @lru_cache(maxsize=1) since simple case functions do not have arguments: from functools import lru_cache @lru_cache ( maxsize = 1 ) def case_a (): # ... (as usual) For case generators you can also use @lru_cache(maxsize=x) , but you will have to set the max size according to the number of generated cases (or None to allow auto-grow). This can be automated: simply use the lru_cache=True parameter and pytest-cases will do it for you: from pytest_cases import CaseData , cases_data , CaseDataGetter , THIS_MODULE , \\ cases_generator # ----------------------CASES-------------------------- # case generator with caching enabled @cases_generator ( \"case {i}\" , i = range ( 3 ), lru_cache = True ) def case_gen ( i ) -> CaseData : print ( \"generating case \" + str ( i )) ins = i outs , err = None , None return ins , outs , err # ----------------------TESTS-------------------------- @cases_data ( module = THIS_MODULE ) def test_a ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it ... @cases_data ( module = THIS_MODULE ) def test_b ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it ... yields: ============================= test session starts ============================= ... collecting ... collected 6 items test_memoize_generators.py::test_a [ case 0 ] PASSED [ 16 % ] generating case 0 0 test_memoize_generators.py::test_a [ case 1 ] PASSED [ 33 % ] generating case 1 1 test_memoize_generators.py::test_a [ case 2 ] PASSED [ 50 % ] generating case 2 2 test_memoize_generators.py::test_b [ case 0 ] PASSED [ 66 % ] 0 test_memoize_generators.py::test_b [ case 1 ] PASSED [ 83 % ] 1 test_memoize_generators.py::test_b [ case 2 ] PASSED [ 100 % ] 2 ========================== 6 passed in 0 .16 seconds =========================== You can see that the second time each case is needed, the cached value is used instead of executing the case generation function again. See doc on lru_cache for implementation details. WARNING if you use case arguments , do not forget to take the additional parameter values into account to estimate the total cache size. Note that the lru_cache= option of @cases_generator is not intelligent enough to handle additional arguments: do not use it, and instead manually apply the @lru_cache decorator.","title":"Caching"},{"location":"usage/advanced/#incremental-tests-with-pytest-steps","text":"Sometimes you wish to execute a series of test steps on the same dataset, and then to move to another one. There are many ways to do this with pytest but some of them are not easy to blend with the notion of 'cases' in an intuitive manner. pytest-cases is compliant with pytest-steps : you can easily create incremental tests and throw your cases on them. This tutorial assumes that you are already familiar with pytest-steps .","title":"Incremental tests with pytest-steps"},{"location":"usage/advanced/#1-if-steps-can-run-with-the-same-data","text":"If all of the test steps require the same data to execute, it is straightforward, both in parametrizer mode (shown below) or in the new pytest steps generator mode (not shown): from pytest_cases import cases_data , CaseDataGetter , THIS_MODULE , CaseData from pytest_steps import test_steps # -------- test cases def case_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) return ins , None , None def case_simple2 () -> CaseData : ins = dict ( a =- 1 , b = 2 ) return ins , None , None # ------- test steps def step_a ( steps_data , ins , expected_o , expected_e ): \"\"\" Step a of the test \"\"\" # Use the three items as usual ... # You can also store intermediate results in steps_data def step_b ( steps_data , ins , expected_o , expected_e ): \"\"\" Step b of the test \"\"\" # Use the three items as usual ... # You can also retrieve intermediate results from steps_data # ------- test suite @test_steps ( step_a , step_b ) @cases_data ( module = THIS_MODULE ) def test_suite ( test_step , case_data : CaseDataGetter , steps_data ): # Get the data for this step ins , expected_o , expected_e = case_data . get () # Execute the step test_step ( steps_data , ins , expected_o , expected_e ) This yields: ============================= test session starts ============================= ... test_p.py::test_suite [ case_simple-step_a ] PASSED [ 25 % ]{ 'a' : 1 , 'b' : 2 } test_p.py::test_suite [ case_simple-step_b ] PASSED [ 50 % ]{ 'a' : 1 , 'b' : 2 } test_p.py::test_suite [ case_simple2-step_a ] PASSED [ 75 % ]{ 'a' : -1, 'b' : 2 } test_p.py::test_suite [ case_simple2-step_b ] PASSED [ 100 % ]{ 'a' : -1, 'b' : 2 } ========================== 4 passed in 0 .13 seconds =========================== You see that for each case data, all steps are executed in order. If you use an IDE it will appear in this intuitive order too: case_simple - step_a - step_b case_simple2 - step_a - step_b If for some reason you wish to invert the order (executing all cases on step a then all cases on step b etc.) simply invert the order of decorators and it will work ( pytest is great!). This is not recommended though, as it is a lot less intuitive. Of course you might want to enable caching so that the cases will be read only once, and not once for each test step.","title":"1- If steps can run with the same data"},{"location":"usage/advanced/#2-if-steps-require-different-data-a-dicts","text":"In real-world usage, each step will probably have different expected output or errors for the same case - except if the steps are very similar. The steps may even need slightly different input, for example the same dataset but in two different formats. This is actually quite straightforward: simply adapt your custom case data format definition! Since pytest-cases does not impose any format for your case functions outputs, you can decide that your case functions return lists, dictionaries, etc. For example you can choose the format proposed by the MultipleStepsCaseData type hint, where each item in the returned inputs/outputs/errors tuple can either be a single element, or a dictionary of name -> element. This allows your case functions to return alternate contents depending on the test step being executed. The example below shows a test suite where the inputs of the steps are the same, but the outputs and expected errors are different. Note that once again the example relies on the legacy \"parametrizer\" mode of pytest-steps, but it would be similar with the new \"generator\" mode. from pytest_cases import cases_data , CaseDataGetter , THIS_MODULE , \\ MultipleStepsCaseData from pytest_steps import test_steps # -------- test cases def case_simple () -> MultipleStepsCaseData : # common input ins = dict ( a = 1 , b = 2 ) # one expected output for each step outs_for_a = 2 , 3 outs_for_b = 5 , 4 outs = dict ( step_check_a = outs_for_a , step_check_b = outs_for_b ) return ins , outs , None def case_simple2 () -> MultipleStepsCaseData : # common input ins = dict ( a =- 1 , b = 2 ) # one expected output for each step outs_for_a = 2 , 3 outs_for_b = 5 , 4 outs = dict ( step_check_a = outs_for_a , step_check_b = outs_for_b ) return ins , outs , None # ------- test steps def step_check_a ( steps_data , ins , expected_o , expected_e ): \"\"\" Step a of the test \"\"\" # Use the three items as usual ... # You can also store intermediate results in steps_data def step_check_b ( steps_data , ins , expected_o , expected_e ): \"\"\" Step b of the test \"\"\" # Use the three items as usual ... # You can also retrieve intermediate results from steps_data # ------- test suite @test_steps ( step_check_a , step_check_b ) @cases_data ( module = THIS_MODULE ) def test_suite ( test_step , case_data : CaseDataGetter , steps_data ): # Get the case data for all steps (sad...) ins , expected_o , expected_e = case_data . get () # Filter it, based on the step name key = test_step . __name__ expected_o = None if expected_o is None else expected_o [ key ] expected_e = None if expected_e is None else expected_e [ key ] # Execute the step test_step ( steps_data , ins , expected_o , expected_e ) There are two main differences with the previous example: in the case_simple and case_simple2 case functions, we choose to provide the expected output and expected error as dictionaries indexed by the test step name when they are non- None . We also choose that the input is the same for all steps, but we could have done otherwise - using a dictionary with step name keys as well. in the final test_suite we use the test step name dictionary key to get the case data contents for a given step . Once again you might want to enable caching in order for the cases to be read only once, and not once for each test step.","title":"2- If steps require different data (A: dicts)"},{"location":"usage/advanced/#3-if-steps-require-different-data-b-arguments-recommended","text":"The above example might seem a bit disappointing as it breaks the philosophy of doing each data access only when it is needed . Indeed everytime a single step is run it actually gets the data for all steps and then has to do some filtering. The style suggested below, making use of case arguments , is probably better: from pytest_cases import cases_data , CaseDataGetter , THIS_MODULE , CaseData from pytest_steps import test_steps # -------- test cases def case_simple ( step_name : str ) -> CaseData : # reuse the same input whatever the step ins = dict ( a = 1 , b = 2 ) # adapt the expected output to the current step if step_name == 'step_check_a' : outs = 2 , 3 elif step_name == 'step_check_b' : outs = 5 , 4 return ins , outs , None def case_simple2 ( step_name : str ) -> CaseData : # reuse the same input whatever the step ins = dict ( a =- 1 , b = 2 ) # adapt the expected output to the current step if step_name == 'step_check_a' : outs = 0 , 3 elif step_name == 'step_check_b' : outs = 1 , 4 return ins , outs , None # ------- test steps def step_check_a ( steps_data , ins , expected_o , expected_e ): \"\"\" Step a of the test \"\"\" # Use the three items as usual ... # You can also store intermediate results in steps_data def step_check_b ( steps_data , ins , expected_o , expected_e ): \"\"\" Step b of the test \"\"\" # Use the three items as usual ... # You can also retrieve intermediate results from steps_data # ------- test suite @test_steps ( step_check_a , step_check_b ) @cases_data ( module = THIS_MODULE ) def test_suite ( test_step , case_data : CaseDataGetter , steps_data ): # Get the data for this particular case ins , expected_o , expected_e = case_data . get ( test_step . __name__ ) # Execute the step test_step ( steps_data , ins , expected_o , expected_e ) Notice that now the test step name is a parameter of the case function . So for each step, only the data relevant to this step is retrieved. Once again you might want to enable caching in order for the cases to be read only once, and not once for each test step. However since the case functions now have arguments, you should not use @lru_cache() directly on the case function but you should put it in a separate subfunction: from pytest_cases import CaseData from functools import lru_cache @lru_cache () def input_for_case_simple (): return dict ( a = 1 , b = 2 ) def case_simple ( step_name : str ) -> CaseData : # reuse the same input whatever the step ins = input_for_case_simple () # adapt the expected output to the current step if step_name == 'step_check_a' : outs = 2 , 3 elif step_name == 'step_check_b' : outs = 5 , 4 return ins , outs , None That way, input_for_case_simple will be cached across the steps. See caching for details.","title":"3- If steps require different data (B: arguments, recommended)"},{"location":"usage/advanced/#advanced-pytest-manual-parametrization","text":"The @cases_data decorator is just syntactic sugar for the following two-steps process, that you may wish to rely on for advanced pytest usages: import pytest from pytest_cases import get_all_cases , get_pytest_parametrize_args # import the module containing the test cases import test_foo_cases # manually list the available cases cases = get_all_cases ( module = test_foo_cases ) # transform into required arguments for pytest (applying the pytest marks if needed) marked_cases , cases_ids = get_pytest_parametrize_args ( cases ) # parametrize the test function manually @pytest.mark.parametrize ( 'case_data' , marked_cases , ids = cases_ids ) def test_with_cases_decorated ( case_data ): ...","title":"Advanced Pytest: Manual parametrization"},{"location":"usage/basics/","text":"Usage basics \u00b6 This page assumes that you have read the initial example . Customizing case names \u00b6 By default the name of the case function is used for the generated test case name. To override this, you can use the @case_name decorator: from pytest_cases import CaseData , case_name @case_name ( \"Simplest\" ) def case_simple_named () -> CaseData : \"\"\" The simplest case but with a custom name using @case_name annotation \"\"\" ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None Pytest marks \u00b6 Pytest marks such as @pytest.mark.skipif can be applied to case functions, the corresponding case will behave as expected. Case generators \u00b6 A case function generator is a function that will generate several test cases, once for every combination of its declared input parameters. The function should have at least one parameter It should be decorated using @cases_generator , passing as keyword arguments one iterable for each parameter Since all generated cases need to have a unique name, you should also provide a name template, following the new string formatting syntax, and referencing all parameters as keyword arguments: from pytest_cases import cases_generator , CaseData @cases_generator ( \"case i={i}, j={j}\" , i = range ( 2 ), j = range ( 3 )) def case_simple_generator ( i , j ) -> CaseData : ins = dict ( a = i , b = j ) outs = i + 1 , j + 1 return ins , outs , None The above case generator will generate one test case for every combination of argument values, so 6 test cases: >>> pytest ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ case i = 0 , j = 0 ] PASSED [ 17 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 0 , j = 1 ] PASSED [ 33 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 0 , j = 2 ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 1 , j = 0 ] PASSED [ 67 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 1 , j = 1 ] PASSED [ 83 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 1 , j = 2 ] PASSED [ 100 % ] ========================== 9 passed in 0 .84 seconds ========================== Handling Exceptions \u00b6 Let's consider the following foo function under test, that may raise an exception: from math import isfinite class InfiniteInput ( Exception ): def __init__ ( self , name ): super ( InfiniteInput , self ) . __init__ ( name ) def __eq__ ( self , other ): return self . args == other . args def foo ( a , b ): \"\"\" An example function to be tested :param a: :param b: :return: \"\"\" if not isfinite ( b ): raise InfiniteInput ( 'b' ) return a + 1 , b + 1 pytest_cases proposes three ways to perform exception checking: either you provide an expected exception type , or an expected exception instance , or an exception validation callable . The example below illustrates the three ways: from math import inf from example import InfiniteInput from pytest_cases import CaseData def case_simple_error_type () -> CaseData : \"\"\" An error case with an exception type \"\"\" ins = dict ( a = 1 , b = \"a_string_not_an_int\" ) err = TypeError return ins , None , err def case_simple_error_instance () -> CaseData : \"\"\" An error case with an exception instance \"\"\" ins = dict ( a = 1 , b = inf ) err = InfiniteInput ( 'b' ) return ins , None , err def case_simple_error_callable () -> CaseData : \"\"\" An error case with an exception validation callable \"\"\" ins = dict ( a = 1 , b = inf ) def is_good_error ( e ): return type ( e ) is InfiniteInput and e . args == ( 'b' ,) return ins , None , is_good_error In order to perform the associated assertions in your test functions, you can simply use the unfold_expected_err utility function bundled in pytest_cases : import pytest from pytest_cases import cases_data , CaseDataGetter , unfold_expected_err from example import foo # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_with_cases_decorated ( case_data : CaseDataGetter ): \"\"\" Example unit test that is automatically parametrized with @cases_data \"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it if expected_e is None : # **** Nominal test **** outs = foo ( ** i ) assert outs == expected_o else : # **** Error test **** # First see what we need to assert err_type , err_inst , err_checker = unfold_expected_err ( expected_e ) # Run with exception capture and type check with pytest . raises ( err_type ) as err_info : foo ( ** i ) # Optional - Additional Exception instance check if err_inst is not None : assert err_info . value == err_inst # Optional - Additional exception instance check if err_checker is not None : err_checker ( err_info . value ) Cases in the same file than Tests \u00b6 It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use the THIS_MODULE constant to refer to the module in which the test function is located: from pytest_cases import CaseData , cases_data , THIS_MODULE , CaseDataGetter def case_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None def case_simple2 () -> CaseData : ins = dict ( a =- 1 , b = 2 ) outs = 0 , 3 return ins , outs , None @cases_data ( module = THIS_MODULE ) def test_with_cases_decorated ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it # ... However WARNING : only the case functions located BEFORE the test function in the module file will be taken into account! To go further \u00b6 Are you at ease with the above concepts ? It's time to move to the intermediate section!","title":"Basics"},{"location":"usage/basics/#usage-basics","text":"This page assumes that you have read the initial example .","title":"Usage basics"},{"location":"usage/basics/#customizing-case-names","text":"By default the name of the case function is used for the generated test case name. To override this, you can use the @case_name decorator: from pytest_cases import CaseData , case_name @case_name ( \"Simplest\" ) def case_simple_named () -> CaseData : \"\"\" The simplest case but with a custom name using @case_name annotation \"\"\" ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None","title":"Customizing case names"},{"location":"usage/basics/#pytest-marks","text":"Pytest marks such as @pytest.mark.skipif can be applied to case functions, the corresponding case will behave as expected.","title":"Pytest marks"},{"location":"usage/basics/#case-generators","text":"A case function generator is a function that will generate several test cases, once for every combination of its declared input parameters. The function should have at least one parameter It should be decorated using @cases_generator , passing as keyword arguments one iterable for each parameter Since all generated cases need to have a unique name, you should also provide a name template, following the new string formatting syntax, and referencing all parameters as keyword arguments: from pytest_cases import cases_generator , CaseData @cases_generator ( \"case i={i}, j={j}\" , i = range ( 2 ), j = range ( 3 )) def case_simple_generator ( i , j ) -> CaseData : ins = dict ( a = i , b = j ) outs = i + 1 , j + 1 return ins , outs , None The above case generator will generate one test case for every combination of argument values, so 6 test cases: >>> pytest ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ case i = 0 , j = 0 ] PASSED [ 17 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 0 , j = 1 ] PASSED [ 33 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 0 , j = 2 ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 1 , j = 0 ] PASSED [ 67 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 1 , j = 1 ] PASSED [ 83 % ] <your_project>/tests/test_foo.py::test_foo [ case i = 1 , j = 2 ] PASSED [ 100 % ] ========================== 9 passed in 0 .84 seconds ==========================","title":"Case generators"},{"location":"usage/basics/#handling-exceptions","text":"Let's consider the following foo function under test, that may raise an exception: from math import isfinite class InfiniteInput ( Exception ): def __init__ ( self , name ): super ( InfiniteInput , self ) . __init__ ( name ) def __eq__ ( self , other ): return self . args == other . args def foo ( a , b ): \"\"\" An example function to be tested :param a: :param b: :return: \"\"\" if not isfinite ( b ): raise InfiniteInput ( 'b' ) return a + 1 , b + 1 pytest_cases proposes three ways to perform exception checking: either you provide an expected exception type , or an expected exception instance , or an exception validation callable . The example below illustrates the three ways: from math import inf from example import InfiniteInput from pytest_cases import CaseData def case_simple_error_type () -> CaseData : \"\"\" An error case with an exception type \"\"\" ins = dict ( a = 1 , b = \"a_string_not_an_int\" ) err = TypeError return ins , None , err def case_simple_error_instance () -> CaseData : \"\"\" An error case with an exception instance \"\"\" ins = dict ( a = 1 , b = inf ) err = InfiniteInput ( 'b' ) return ins , None , err def case_simple_error_callable () -> CaseData : \"\"\" An error case with an exception validation callable \"\"\" ins = dict ( a = 1 , b = inf ) def is_good_error ( e ): return type ( e ) is InfiniteInput and e . args == ( 'b' ,) return ins , None , is_good_error In order to perform the associated assertions in your test functions, you can simply use the unfold_expected_err utility function bundled in pytest_cases : import pytest from pytest_cases import cases_data , CaseDataGetter , unfold_expected_err from example import foo # import the module containing the test cases import test_foo_cases @cases_data ( module = test_foo_cases ) def test_with_cases_decorated ( case_data : CaseDataGetter ): \"\"\" Example unit test that is automatically parametrized with @cases_data \"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it if expected_e is None : # **** Nominal test **** outs = foo ( ** i ) assert outs == expected_o else : # **** Error test **** # First see what we need to assert err_type , err_inst , err_checker = unfold_expected_err ( expected_e ) # Run with exception capture and type check with pytest . raises ( err_type ) as err_info : foo ( ** i ) # Optional - Additional Exception instance check if err_inst is not None : assert err_info . value == err_inst # Optional - Additional exception instance check if err_checker is not None : err_checker ( err_info . value )","title":"Handling Exceptions"},{"location":"usage/basics/#cases-in-the-same-file-than-tests","text":"It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use the THIS_MODULE constant to refer to the module in which the test function is located: from pytest_cases import CaseData , cases_data , THIS_MODULE , CaseDataGetter def case_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None def case_simple2 () -> CaseData : ins = dict ( a =- 1 , b = 2 ) outs = 0 , 3 return ins , outs , None @cases_data ( module = THIS_MODULE ) def test_with_cases_decorated ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it # ... However WARNING : only the case functions located BEFORE the test function in the module file will be taken into account!","title":"Cases in the same file than Tests"},{"location":"usage/basics/#to-go-further","text":"Are you at ease with the above concepts ? It's time to move to the intermediate section!","title":"To go further"},{"location":"usage/intermediate/","text":"Intermediate Usage \u00b6 You might feel a bit stuck when using only the basics. In particular you might feel frustrated by having to put all cases for the same function in the same dedicated file. In this section we see that this is not mandatory at all: pytest-cases offers flexibility mechanisms to organize the files the way you wish. Gathering Cases from different files \u00b6 You can gather cases from different files (modules) in the same test function: from pytest_cases import CaseDataGetter , cases_data # the 2 modules containing cases from . import shared_cases , shared_cases2 @cases_data ( module = [ shared_cases , shared_cases2 ]) def test_bar ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only # ... Storing Cases with different purposes in the same file \u00b6 This is the opposite of the above: sometimes it would just be tideous to create a dedicated cases file for every single test function, you just want to be able to put all of your cases in the same place, even if they are not used in the same test. To do this we need to associate test functions with test cases in a more fine-grain way. a- Hardcoded cases list \u00b6 On the test functions side, you can precisely select the required cases in @cases_data using cases=<case or list of cases> from pytest_cases import CaseDataGetter , cases_data # the module containing the cases above from .shared_cases import case1 , case2 , case3 # the 2 functions that we want to test from mycode import foo , bar @cases_data ( cases = [ case1 , case2 ]) def test_foo ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'foo'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = foo ( ** i ) assert outs == expected_o @cases_data ( cases = case3 ) def test_bar ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'bar'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = bar ( ** i ) assert outs == expected_o In the example above, test_foo will be applied on case1 and case2 , while test_bar will be applied on case3 . They can live in the same file or not, it does not matter. b- Simple: declare a test target \u00b6 The simplest non-hardcoded approach is to use a common reference so that each test function finds the appropriate cases. The function or class under test (in other words, the \"test target\") might be a good idea to serve this purpose. On the cases side, simply use the @test_target decorator: from pytest_cases import CaseData , test_target # the 2 functions that we want to test from mycode import foo , bar # a case only to be used when function foo is under test @test_target ( foo ) def case_foo_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None # a case only to be used when function bar is under test @test_target ( bar ) def case_bar_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None On the test functions side, filter the cases in @cases_data using has_tag=<target> : from pytest_cases import CaseDataGetter , cases_data # the module containing the cases above from . import shared_cases # the 2 functions that we want to test from mycode import foo , bar @cases_data ( module = shared_cases , has_tag = foo ) def test_foo ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'foo'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = foo ( ** i ) assert outs == expected_o @cases_data ( module = shared_cases , has_tag = bar ) def test_bar ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'bar'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = bar ( ** i ) assert outs == expected_o Of course this does not prevent other test functions to use all cases by not using any filter. c- Advanced: Tagging & Filtering \u00b6 The above example is just a particular case of tag put on a case, and filter put on the test function. You can actually put several tags on the cases, not only a single one (like @test_target does): from pytest_cases import CaseData , case_tags # a case with two tags @case_tags ( bar , 'fast' ) def case_multitag_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None Test functions can use two things to perform their selection: the has_tag parameter, has seen above the filter parameter, that should be a callable taking as input a list of tags and returning a boolean. If both are provided, a AND will be applied. For example: from pytest_cases import THIS_MODULE , cases_data , CaseDataGetter def has_a_or_b ( tags ): return 'a' in tags or 'b' in tags @cases_data ( module = THIS_MODULE , filter = has_a_or_b ) def test_with_cases_a_or_b ( case_data : CaseDataGetter ): # ... Or with a lambda function: from pytest_cases import THIS_MODULE , cases_data , CaseDataGetter @cases_data ( module = THIS_MODULE , filter = lambda tags : 'a' in tags or 'b' in tags ) def test_with_cases_a_or_b ( case_data : CaseDataGetter ): # ... Or with a mini lambda expression: from pytest_cases import THIS_MODULE , cases_data , CaseDataGetter from mini_lambda import InputVar , _ tags = InputVar ( 'tags' , list ) @cases_data ( module = THIS_MODULE , filter = _ ( tags . contains ( 'a' ) | tags . contains ( 'b' ))) def test_with_cases_a_or_b ( case_data : CaseDataGetter ): # ... To go further \u00b6 Are you at ease with the above concepts ? It's time to move to the advanced section!","title":"Intermediate"},{"location":"usage/intermediate/#intermediate-usage","text":"You might feel a bit stuck when using only the basics. In particular you might feel frustrated by having to put all cases for the same function in the same dedicated file. In this section we see that this is not mandatory at all: pytest-cases offers flexibility mechanisms to organize the files the way you wish.","title":"Intermediate Usage"},{"location":"usage/intermediate/#gathering-cases-from-different-files","text":"You can gather cases from different files (modules) in the same test function: from pytest_cases import CaseDataGetter , cases_data # the 2 modules containing cases from . import shared_cases , shared_cases2 @cases_data ( module = [ shared_cases , shared_cases2 ]) def test_bar ( case_data : CaseDataGetter ): # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only # ...","title":"Gathering Cases from different files"},{"location":"usage/intermediate/#storing-cases-with-different-purposes-in-the-same-file","text":"This is the opposite of the above: sometimes it would just be tideous to create a dedicated cases file for every single test function, you just want to be able to put all of your cases in the same place, even if they are not used in the same test. To do this we need to associate test functions with test cases in a more fine-grain way.","title":"Storing Cases with different purposes in the same file"},{"location":"usage/intermediate/#a-hardcoded-cases-list","text":"On the test functions side, you can precisely select the required cases in @cases_data using cases=<case or list of cases> from pytest_cases import CaseDataGetter , cases_data # the module containing the cases above from .shared_cases import case1 , case2 , case3 # the 2 functions that we want to test from mycode import foo , bar @cases_data ( cases = [ case1 , case2 ]) def test_foo ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'foo'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = foo ( ** i ) assert outs == expected_o @cases_data ( cases = case3 ) def test_bar ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'bar'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = bar ( ** i ) assert outs == expected_o In the example above, test_foo will be applied on case1 and case2 , while test_bar will be applied on case3 . They can live in the same file or not, it does not matter.","title":"a- Hardcoded cases list"},{"location":"usage/intermediate/#b-simple-declare-a-test-target","text":"The simplest non-hardcoded approach is to use a common reference so that each test function finds the appropriate cases. The function or class under test (in other words, the \"test target\") might be a good idea to serve this purpose. On the cases side, simply use the @test_target decorator: from pytest_cases import CaseData , test_target # the 2 functions that we want to test from mycode import foo , bar # a case only to be used when function foo is under test @test_target ( foo ) def case_foo_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None # a case only to be used when function bar is under test @test_target ( bar ) def case_bar_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None On the test functions side, filter the cases in @cases_data using has_tag=<target> : from pytest_cases import CaseDataGetter , cases_data # the module containing the cases above from . import shared_cases # the 2 functions that we want to test from mycode import foo , bar @cases_data ( module = shared_cases , has_tag = foo ) def test_foo ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'foo'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = foo ( ** i ) assert outs == expected_o @cases_data ( module = shared_cases , has_tag = bar ) def test_bar ( case_data : CaseDataGetter ): \"\"\" This test will only be executed on cases tagged with 'bar'\"\"\" # 1- Grab the test case data i , expected_o , expected_e = case_data . get () # 2- Use it: nominal test only assert expected_e is None outs = bar ( ** i ) assert outs == expected_o Of course this does not prevent other test functions to use all cases by not using any filter.","title":"b- Simple: declare a test target"},{"location":"usage/intermediate/#c-advanced-tagging-filtering","text":"The above example is just a particular case of tag put on a case, and filter put on the test function. You can actually put several tags on the cases, not only a single one (like @test_target does): from pytest_cases import CaseData , case_tags # a case with two tags @case_tags ( bar , 'fast' ) def case_multitag_simple () -> CaseData : ins = dict ( a = 1 , b = 2 ) outs = 2 , 3 return ins , outs , None Test functions can use two things to perform their selection: the has_tag parameter, has seen above the filter parameter, that should be a callable taking as input a list of tags and returning a boolean. If both are provided, a AND will be applied. For example: from pytest_cases import THIS_MODULE , cases_data , CaseDataGetter def has_a_or_b ( tags ): return 'a' in tags or 'b' in tags @cases_data ( module = THIS_MODULE , filter = has_a_or_b ) def test_with_cases_a_or_b ( case_data : CaseDataGetter ): # ... Or with a lambda function: from pytest_cases import THIS_MODULE , cases_data , CaseDataGetter @cases_data ( module = THIS_MODULE , filter = lambda tags : 'a' in tags or 'b' in tags ) def test_with_cases_a_or_b ( case_data : CaseDataGetter ): # ... Or with a mini lambda expression: from pytest_cases import THIS_MODULE , cases_data , CaseDataGetter from mini_lambda import InputVar , _ tags = InputVar ( 'tags' , list ) @cases_data ( module = THIS_MODULE , filter = _ ( tags . contains ( 'a' ) | tags . contains ( 'b' ))) def test_with_cases_a_or_b ( case_data : CaseDataGetter ): # ...","title":"c- Advanced: Tagging &amp; Filtering"},{"location":"usage/intermediate/#to-go-further","text":"Are you at ease with the above concepts ? It's time to move to the advanced section!","title":"To go further"}]}